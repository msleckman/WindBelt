---
title: "WindBelt Sentiment Analysis and Regressions"
author: "WindBelt GP"
date: "February 20, 2019"
output: Word Document
---

```{r setup2, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


#### Initial read in files: 
Main:
1. Ventyx dataset: variables included: "ProjectName", "ProjectDeveloper", "ProjectOwner", "State", "County", "State_County", "County, State", "PopDensity_mi"          "Household_MeanIncome", "Household_MedianIncome",  "View_Score", "Latitude", "Longitude", "Capacity", "CurrentPhase", "ArtificialEnd", "TimelineDays", "lr_tif" (low risk areas)         "document" (pdf associated), "H_L_W" (whether in high or low risk area - essentially lr_tif in binary form)

2. google_df: "PDFName" ("projectname_developer_numberofNUarticles.pdf"), "ProjectName", "ProjectDeveloper", "State", "NegativeWordCount", "FullText"      

Additional:
3. Memberships.csv: variables include: "zip", "population", "members_nrdc", "members_nwf", "members_tnc", "average", "average_rounded", "rate"   

#### Outline of google news text analysis (checked if done):
1. Read in google news file with text + read in ventyx
2. Create a df with every project and the articles associated with the project
3. Create df with google article text unested (using unest_tokens),such that each row is a word.x
5. Get word count of specific words through group_by()
6. Get a weighted mean based on the words available and the count 
7. Conduct sentiment analysis merge with AFINN dictionary and get a mean sentiment score per project  x 
8. Get hits and number of hits of different 'negative words' x
 
*tidytext::tokenize function - every element in list become df. rbind dfs 
 str_count() how many times does a search term come up 
 str_match()
 regex() 

### Processing Preparation 
```{r load_packages}

##-- If needed

# install.packages("devtools")
# install.packages("tidytext")
# library(tm.plugin.lexisnexis)
# install.packages("gsubfn")
# install.packages("broom")
# install.packages("stringr")
# install.packages("purrr")
# install.packages("rlang")
  install.packages("pscl")
# install.packages("dotwhisker")
# install.packages("jtools")
# install.packages("sjPlot")
  install.packages("InformationValue")
# library(knitr)
# library(pdftools)
# library(tm)

```

```{r libraries}

library(tidyverse)
library(googledrive)
library(purrr)
library(readr)
library(stringr)
library(gsubfn)
library(devtools)
library(dplyr)
library(tidytext)
library(broom)
library(data.table)
library(ggplot2)
library(dotwhisker)
library(jtools)
library(sjPlot)
library(jtools)
library(ggeffects)
library(pscl)
library(car)
library(InformationValue)
library(boot)
library(stargazer)
library(pwr)
library(caret)
library(plyr)
library(gt)

```

```{r functions}

# Function to download files from googledrive - such as google_scraping csv, ventyx csv, 

gdrive_downloader <- function(templates_dribble, local_folder){
  # download files
  
  for (i in 1:nrow(templates_dribble)){
    drive_download(as_id(templates_dribble$id[[i]]), 
                   file.path(local_folder, templates_dribble$name[[i]]),
                   overwrite = F)
  }
    #check if overwrite is needed here, normally, if these files have already been downloaded the your desktop, it will not download.
}

normalize <- function(x){
  norm_x <- (x - mean(x, na.rm = T))/sd(x, na.rm =T)
  return(norm_x)
}

```

```{r files_connections_gdrive}

##### Ventyx ####
## create a folder in desktop to store files to read_in to this script
base_path <- "H:/Desktop/"
sub_path <- "CSVs_for_R_scripts_FebUpdates"
dir.create(file.path(base_path, sub_path), showWarnings = FALSE)


##Pulling relevant csv from Google drive to desktop

CSVs_for_R_script_id  <- "14x4uXza2wmtT17gSCJZK3L2Z0Yy7L1Kn"
CSVs_for_R_script_folder <- googledrive::drive_ls(googledrive::as_id(CSVs_for_R_script_id))
desktop_path <- "~/Desktop/CSVs_for_R_scripts_FebUpdates/"

gdrive_downloader(CSVs_for_R_script_folder, desktop_path)
  # If files already exist, you will get a error - please ignore

```


```{r readIn_csv}

#read_in files
# ventyx csv
ventyx_projects <- read_csv("~/Desktop/CSVs_for_R_scripts_FebUpdates/Copy_of_ventyx_converted_02192019_view_pop_income_lowImpact.csv")
names(ventyx_projects)

# google scraping csv
google_df <- read_csv("~/Desktop/CSVs_for_R_scripts_FebUpdates/Copy of google_scraping_02-18-19.csv")
names(google_df)
# memberships csv
zip_conversion <- read_csv("~/Desktop/CSVs_for_R_scripts_FebUpdates/Copy_zip_code_database.csv")
memberships <- read_csv("~/Desktop/CSVs_for_R_scripts_FebUpdates/Copy_membership_data_combined.csv")

```

```{r readIn_csv_Alex}
#Only run this if you're Alex
ventyx_projects <- read_csv("C:/Users/airvi/Documents/Bren/GroupProject/MainData/ventyx_converted_02-19-2019_view_pop_income_lowImpact.csv")
google_df <- read_csv("C:/Users/airvi/Documents/Bren/GroupProject/MainData/google_scraping_02-18-19.csv")
zip_conversion <- read_csv("C:/Users/airvi/Documents/Bren/GroupProject/MainData/zip_code_database.csv")
memberships <- read_csv("C:/Users/airvi/Documents/Bren/GroupProject/MainData/membership_data_combined.csv")
```


```{r csv_quick_edits_fixes}

names(ventyx_projects)

#To change the 0-1 definition to Low-High
ventyx_df <- mutate(ventyx_projects, H_L_W = ifelse(lr_tif>0, "Low", "High"))

#Remove OID column
ventyx_df <- ventyx_df %>% 
  select(-OID)

#Remove whitespace for name matching later one
ventyx_df$ProjectNam <- gsub(" ", "", ventyx_df$ProjectNam, fixed = TRUE)
# ventyx_df$ <- gsub(" ", "", ventyx_df$document, fixed = TRUE)

```

#### Create initial dataframes that include document name and text
```{r dataframe_creation}

#This creates a data frame of just the project name and full text for each Google News article
#Each row is the project name with the full article text

projects_google <- data_frame(ProjectNam = google_df$ProjectName, text = google_df$FullText)

# head(projects_google)

```

#### Split text by word (unnest_tokens())
```{r word_split}

#Dataset with each word in a row associated with its project source 

projects_words_google <- projects_google %>% 
  tidytext::unnest_tokens(output = word, input = text, token = "words", to_lower = F)

# head(projects_words_google)

```

#### Group words by project and summarize by frequency
```{r group_by}

projects_words_count_google <- projects_words_google %>%
  group_by(ProjectNam, word) %>% 
  summarise(count = n())

  # Counts the number of time a specific words is found in the article

# View(projects_words_count_google)

```

#### Sentiment dictionaries
```{r sentiment_dictionaries}

# Using 'afinn' vs. 'nrc sentiment tests.

get_sentiments("afinn") # associates word with a sentiment score
#afinn scores/ranks from -5 to +5 for positive or negative sentiment.

get_sentiments("nrc") # associated word with another sentiment feeling word

# We want scores not categorized words, so we will use AFINN for now.

```

#### Bind Sentiments
```{r bind_sentiment}

projects_score_bind_google <- projects_words_count_google %>%
  left_join(get_sentiments("afinn"), by = "word") %>% 
  filter(score != "NA")

# View(projects_score_bind_google[grep("Osborn*", projects_score_bind_google$document),])  
  # filter(document == "^Osborn*"))

# Note: Many of the scores per words are NA simply because that word does not exist. 

```

#### Determine Project Scores
```{r projectscores}

#To determine the total score for each project

total_sentiment_with_stats_google <- projects_score_bind_google %>% 
  #filter(score !="NA") %>% 
  group_by(ProjectNam) %>% 
  summarise(totals = weighted.mean(score, w = count),
            standard_dev = sd(score), 
            variance = var(score))

total_sentiment_google <- projects_score_bind_google %>% 
  #filter(score !="NA") %>% 
  group_by(ProjectNam) %>% 
  summarise(totals = mean(score))
  
# View(total_sentiment_with_stats_google)
# View(total_sentiment_google)

```

#### Combine sentiment scores with original data
```{r combinewithsen}

#Convert to data frame
total_sentiment_google_df = as.data.frame(total_sentiment_with_stats_google)
#sample_n(total_sentiment_google_df, 20)

#Remove white space in order to perform merge (google data has extra white spaces)
total_sentiment_google_df$ProjectNam <- gsub(" ", "", total_sentiment_google_df$ProjectNam, fixed = TRUE)

#Merge with original data
ventyx_df_google_Sen <- merge(ventyx_df, total_sentiment_google_df, by = "ProjectNam", all = TRUE)

#head(ventyx_df_google_Sen)
#names(ventyx_df_google_Sen)

#Rename "totals" columns
ventyx_df_google_Sen <- rename(ventyx_df_google_Sen, Google_Sentiment=totals)

#names(ventyx_df_google_Sen)
#sample_n(ventyx_df_google_Sen[,39:42], 20)

```

#### Convert sentiment scores to positive or negative in new column
```{r pos_or_neg_sen}

ventyx_df_google_Sen <- mutate(ventyx_df_google_Sen, Sign_Google = ifelse(Google_Sentiment>0, "Positive", "Negative"))

```

#### Add environmental memberships data
```{r env_members}

#Make sure all zip codes are 5 digits
memberships$zip <- sprintf("%05d", memberships$zip)

#Merge memberships data with county
zip_county_merge <- merge(zip_conversion, memberships, by = "zip")

#Remove the word county, parish, and municipality, then trim any trailing whitespace
zip_county_merge$county <- gsub("County","",zip_county_merge$county)
zip_county_merge$county <- gsub("Parish","",zip_county_merge$county)
zip_county_merge$county <- gsub("Municipality","",zip_county_merge$county)
zip_county_merge$county <- trimws(zip_county_merge$county, "r")

#Group by county and sum all of the members for environmental organizations
zip_county_merge_aggregate <- zip_county_merge %>% 
  group_by(county) %>% 
  summarise(members_env = sum(members_nrdc,members_nwf,members_tnc))

#Merge with ventyx dataset and set NA membership values to zero
ventyx_df_google_Sen <- merge(ventyx_df_google_Sen, zip_county_merge_aggregate, by.x = "County", by.y = "county", all.x = TRUE)
ventyx_df_google_Sen$members_env[is.na(ventyx_df_google_Sen$members_env)] <- 0
 
```

#### Regression Prep

```{r finalize_ventyx_for_regression}

# Make NA scores 0
ventyx_df_google_Sen <- ventyx_df_google_Sen %>% 
  mutate(Google_Sentiment = ifelse(is.na(Google_Sentiment),0,Google_Sentiment)) %>% 
  mutate(Canceled = ifelse(OnlyCancel == "Yes",1,0))

ventyx_df_google_Sen <- ventyx_df_google_Sen %>% 
  filter(Capacity > 0) %>% 
  filter(TimelineDa > 0) %>% 
  filter(!lr_tif < 0) %>% 
  filter(!View_Score < 0) %>% 
  filter(!is.na(Household_))

```

```{r organize_for_regressions}

ventyx_df_google_Sen$H_L_W <- as.factor(ventyx_df_google_Sen$H_L_W)

ventyx_df_google_Sen$H_L_W <- relevel(ventyx_df_google_Sen$H_L_W, ref = "High")

ventyx_df_google_Sen$PopDensity <- as.numeric(ventyx_df_google_Sen$PopDensity)

ventyx_df_google_Sen$Household_ <- as.numeric(ventyx_df_google_Sen$Household_)

ventyx_df_google_Sen$View_Score <- as.numeric(ventyx_df_google_Sen$View_Score)

ventyx_df_google_Sen$Sign_Google <- as.factor(ventyx_df_google_Sen$Sign_Google)

ventyx_df_google_Sen$OnlyCancel <- as.factor(ventyx_df_google_Sen$OnlyCancel)

ventyx_df_google_Sen$State <- as.factor(ventyx_df_google_Sen$State)

ventyx_df_google_Sen$State <- relevel(ventyx_df_google_Sen$State, ref = "Texas")


### adding normalization to df 
# 
# ventyx_df_NU_google_Sen_norm <- ventyx_df_NU_google_Sen %>% 
#   select(-starts_with("X")) %>% 
#   mutate(Capacity_2 = normalize(Capacity),
#          View_Score_2 = normalize(View_Score),
#          Google_Sentiment_2 = normalize(Google_Sentiment),
#          NU_Sentiment_2 = normalize(NU_Sentiment),
#          PopDensity_mi_2 = normalize(PopDensity_mi),
#          Household_MedianIncome_2 = normalize(Household_MedianIncome),
#          members_env_2 = normalize(members_env)) 
  
```

```{r prep}

renamed_variables <- c(View_Score = "View Score",
                        Google_Sentiment = "Google News Publicity score",
                        NU_Sentiment = "Nexis Uni Publicity score",
                        PopDensity_mi = "Population Density per m2",
                        Household_MedianIncome = "Household Median Income",
                        Capacity = "Capacity",
                        H_L_WLow = "High vs. low impact area"
                        # H_L_W*NU_Sentiment = "site vs. sentiment interaction"
                       )

ventyx_Operating <- ventyx_df_google_Sen %>% 
  filter(OnlyCancel == "No" & Artificial == "No" & OperatingD == "No")

ventyx_Operating_Canceled <- ventyx_df_google_Sen %>% 
  filter(Artificial == "No" & OperatingD == "No")

ventyx_Canceled <- ventyx_df_google_Sen %>% 
  filter(OnlyCancel == "Yes" & OperatingD == "No" & Artificial == "No")

Ventyx_Operating_NoZeros <- ventyx_df_google_Sen %>% 
  filter(OnlyCancel == "No" & Artificial == "No" & OperatingD == "No") %>% 
  filter(Google_Sentiment != 0)

Ventyx_Operating_Canceled_NoZeros <- ventyx_df_google_Sen %>% 
  filter(Artificial == "No" & OperatingD == "No") %>% 
  filter(Google_Sentiment != 0) %>% 
  filter(State != "Arkansas")

ventyx_Truncated <- ventyx_df_google_Sen %>% 
  filter(Artificial == "Yes")

#ventyx_Operating_Canceled$OnlyCancel <- relevel(ventyx_Operating_Canceled$OnlyCancel, ref = "No")
#write_csv(ventyx_Operating_Canceled, "ventyx_Operating_Canceled_03-11-2019.csv")
#write_csv(ventyx_df_google_Sen, "ventyx_all_03-11-2019.csv")

```

```{r descriptive_stats}

#Averages of operating projects
mean(ventyx_Operating$TimelineDa)
mean(ventyx_Operating$Google_Sentiment)
mean(ventyx_Operating$Capacity)
mean(ventyx_Operating$View_Score)
mean(ventyx_Operating$PopDensity)
mean(ventyx_Operating$Household_)
mean(ventyx_Operating$members_env)
sum(ventyx_Operating$H_L_W == "Low")/sum(ventyx_Operating$H_L_W == "High")

#Averages of canceled projects
mean(ventyx_Canceled$TimelineDa)
mean(ventyx_Canceled$Google_Sentiment)
mean(ventyx_Canceled$Capacity)
mean(ventyx_Canceled$View_Score)
mean(ventyx_Canceled$PopDensity)
mean(ventyx_Canceled$Household_)
mean(ventyx_Canceled$members_env)
sum(ventyx_Canceled$H_L_W == "Low")/sum(ventyx_Canceled$H_L_W == "High")

#Sum of projects with articles
sum(ventyx_Operating_Canceled$Google_Sentiment != 0)

#Average number of articles per project
google_df$Sum <- table(google_df$ProjectName)[google_df$ProjectName]
mean(google_df$Sum)

#Evaluating the average project
mean_project <- c(1149.821, 65.11366, 0.2926, 57.90, 50516.88, 117.08, "High", "Texas", 1346.125)
mean_project <- as.data.frame(mean_project)
average_project <- mean_project # change name of df so that row and df aren't the same name
average_project <- t(average_project)
colnames(average_project) <- c("TimelineDa", "View_Score", "Google_Sentiment", "PopDensity", "Household_", "Capacity", "H_L_W", "State", "members_env")

average_project <- as.data.frame(average_project)
average_project[1:6] <- lapply(average_project, function(x) as.numeric(as.character(x)))
average_project[9] <- lapply(average_project, function(x) as.numeric(as.character(x)))
average_project$H_L_W <- as.factor(average_project$H_L_W)
average_project$State <- as.factor(average_project$State)
```


#### Run Regressions

```{r linear_regression_all}

#Regression of all operating / canceled projects with timeline as the dependent variable
linear_reg_all <- lm(TimelineDa ~ View_Score + Google_Sentiment + PopDensity + Household_ + Capacity + H_L_W + State + Google_Sentiment*H_L_W + Google_Sentiment*OnlyCancel + members_env + OnlyCancel, data=ventyx_Operating_Canceled)

summary(linear_reg_all)
jtools::summ(linear_reg_all)

```

```{r linear_regression_sentiment}

#Regression of all operating / canceled projects with sentiment as the dependent variable
linear_reg_sentiment <- lm(Google_Sentiment ~ View_Score + TimelineDa + PopDensity + Household_ + Capacity + H_L_W + State, data=ventyx_Operating_Canceled)

summary(linear_reg_sentiment)
jtools::summ(linear_reg_sentiment)

```


```{r logistic_regression_all}

#For the purposes of this regression, we first want to subset our data into a section that we used to train the model, and a section for testing the predicted probabilities later on.
ventyx_shuffled <- ventyx_Operating_Canceled[sample(nrow(ventyx_Operating_Canceled)),] #to shuffle dataset
train <- ventyx_shuffled[1:868,]
test <- ventyx_shuffled[801:868,]

#To test the reversal of values if we switch "High" impact areas to "Low" and vice verse
test_reversed <- test %>% 
  mutate(H_L_W = ifelse(H_L_W == "High", "Low","High"))

#Next, we run the logistic regression
logistic_reg_all <- glm(Canceled ~ TimelineDa + View_Score + Google_Sentiment + PopDensity + Household_ + Capacity + H_L_W + members_env + Google_Sentiment*H_L_W + State, family = binomial, data = train)

#View model results
summary(logistic_reg_all) #coefficients represent log odds
exp(coef(logistic_reg_all)) #exponentiated coefficients

#Run anova to compare null deviance vs. residual deviance. The greater the difference the better.
anova(logistic_reg_all, test="Chisq")

#Diagnostic plots
glm.diag.plots(logistic_reg_all)

#To view the logistic model equivalent of R-squared
pR2(logistic_reg_all)

#Check for multicollinearity
vif(logistic_reg_all) #low multicollinearity

#Now we predict the probability of the test projects being canceled
pred_probs <- predict(logistic_reg_all, newdata=test, type='response', se.fit = TRUE)
pred_probs_df <- data.frame(test, pred_probs$fit, pred_probs$se.fit)
pred_probs_df <- pred_probs_df %>% 
  rename(Probability = pred_probs.fit) %>% 
  rename(Publicity = Google_Sentiment)

#Now predict probabilities for reversed values
pred_probs_reversed <- predict(logistic_reg_all, newdata=test_reversed, type='response', se.fit = TRUE)
pred_probs_df_reversed <- data.frame(test, pred_probs_reversed$fit, pred_probs_reversed$se.fit)
pred_probs_df_reversed <- pred_probs_df_reversed %>% 
  rename(Probability = pred_probs_reversed.fit) %>% 
  rename(Publicity = Google_Sentiment)

#Probability means
mean(pred_probs_df$Probability)
mean(pred_probs_df_reversed$pred_probs_reversed.fit)

# pred_low <- pred_probs_df %>% 
#   filter(H_L_W == "Low")
# pred_high <- pred_probs_df %>% 
#   filter(H_L_W == "High")
# mean(pred_low$Probability)
# mean(pred_high$Probability)

#To evaluate the misclassification of predicted cancelations and actual cancelations -- essentially, how accurate are the predictions?
optCutOff <- optimalCutoff(test$Canceled, pred_probs$fit)
misClassError(test$Canceled, pred_probs$fit, threshold = optCutOff) #~14% misclassification error, so overall pretty accurate

#Plot ROC, which traces the percentage of true positives accurately predicted by the model as the prediction probability cutoff is lowered from 1 to 0.
plotROC(test$Canceled, pred_probs$fit) #the graph has a steep curve, which indicated a good quality model

#Measure concordance, which measures how well high probability scores match up with cancelation values, and vice versa.
Concordance(test$Canceled, pred_probs$fit) #a concordance of ~0.9 is a high quality model

```

```{r logistic_regression_all_NoZeroes}

#For the purposes of this regression, we first want to subset our data into a section that we used to train the model, and a section for testing the predicted probabilities later on.
ventyx_shuffled_NoZeroes <- Ventyx_Operating_Canceled_NoZeros[sample(nrow(Ventyx_Operating_Canceled_NoZeros)),] #to shuffle dataset
train_NoZeroes <- ventyx_shuffled_NoZeroes[1:275,]
test_NoZeroes <- ventyx_shuffled_NoZeroes[207:276,]

#Next, we run the logistic regression
logistic_reg_all_NoZeroes <- glm(Canceled ~ TimelineDa + View_Score + Google_Sentiment + PopDensity + Household_ + Capacity + H_L_W + members_env + Google_Sentiment*H_L_W + State, family = binomial, data = train_NoZeroes)

#View model results
summary(logistic_reg_all_NoZeroes) #coefficients represent log odds
exp(coef(logistic_reg_all_NoZeroes)) #exponentiated coefficients

#Run anova to compare null deviance vs. residual deviance. The greater the difference the better.
anova(logistic_reg_all_NoZeroes, test="Chisq")

#Diagnostic plots
glm.diag.plots(logistic_reg_all_NoZeroes)

#To view the logistic model equivalent of R-squared
pR2(logistic_reg_all_NoZeroes)

#Check for multicollinearity
vif(logistic_reg_all_NoZeroes) #low multicollinearity except for state

#Now we predict the probability of the test projects being canceled
pred_probs_NoZeroes <- predict(logistic_reg_all_NoZeroes, newdata=test_NoZeroes, type='response', se.fit = TRUE)
pred_probs_df_NoZeroes <- data.frame(test_NoZeroes, pred_probs_NoZeroes$fit, pred_probs_NoZeroes$se.fit)
pred_probs_df_NoZeroes <- pred_probs_df_NoZeroes %>% 
  rename(Probability = pred_probs_NoZeroes.fit) %>% 
  rename(Publicity = Google_Sentiment)

#To evaluate the misclassification of predicted cancelations and actual cancelations -- essentially, how accurate are the predictions?
optCutOff <- optimalCutoff(test_NoZeroes$Canceled, pred_probs_NoZeroes$fit)
misClassError(test_NoZeroes$Canceled, pred_probs_NoZeroes$fit, threshold = optCutOff) #19% misclassification error, so overall pretty accurate

#Plot ROC, which traces the percentage of true positives accurately predicted by the model as the prediction probability cutoff is lowered from 1 to 0.
plotROC(test_NoZeroes$Canceled, pred_probs_NoZeroes$fit) #the graph has a steep curve, but not as steep as the model with zeroes included

#Measure concordance, which measures how well high probability scores match up with cancelation values, and vice versa.
Concordance(test_NoZeroes$Canceled, pred_probs_NoZeroes$fit) #a concordance of ~0.65 which is pretty good
```

```{r bivariate_regression}
#This is to test our sample size
ventyx_Unfiltered <- read_csv("ventyx_unfiltered_03-19-2019.csv")
colnames(ventyx_Unfiltered) <- c("ProjectNam","ProjectDev","ProjectOwn","State","County","Latitude","Longitude","Capacity","CurrentPha","TimelineDa","OnlyCancel")

unfiltered_capacity <- ventyx_Unfiltered$Capacity
filtered_capacity <- ventyx_df_google_Sen$Capacity
var.test(unfiltered_capacity, filtered_capacity) #variances are not equal (p = 0.29)
wilcox.test(unfiltered_capacity, filtered_capacity)

unfiltered_timeline <- ventyx_Unfiltered$TimelineDa
filtered_timeline <- ventyx_df_google_Sen$TimelineDa
var.test(unfiltered_timeline, filtered_timeline) #variances are not equal (p = 0.29)
wilcox.test(unfiltered_timeline, filtered_timeline)

unfiltered_canceled <- ifelse(ventyx_Unfiltered$OnlyCancel == "Yes",1,0)
filtered_canceled <- ifelse(ventyx_df_google_Sen$OnlyCancel == "Yes",1,0)
var.test(unfiltered_canceled, filtered_canceled) #variances are not equal (p = 0.29)
wilcox.test(unfiltered_canceled, filtered_canceled)

logistic_reg_bivariate <- glm(Canceled ~  H_L_W, family = binomial, data = ventyx_df_google_Sen)
summary(logistic_reg_bivariate)


varImp(logistic_reg_bivariate)

cohen.ES(test = "f2", size = "medium")
```


```{r Other_predictions}

#Predicting cancelation for truncated projects
pred_probs_trunc <- predict(logistic_reg_all, newdata=ventyx_Truncated, type='response', se.fit = TRUE)
pred_probs_trunc_df <- data.frame(ventyx_Truncated, pred_probs_trunc$fit, pred_probs_trunc$se.fit)
pred_probs_trunc_df <- pred_probs_trunc_df %>% 
  rename(Probability = pred_probs_trunc.fit) %>% 
  rename(Publicity = Google_Sentiment)

mean(pred_probs_trunc_df$Probability)

#Predicting cancelation for the average project
mean_pred_probs <- predict(logistic_reg_all, newdata=average_project, type='response', se.fit = TRUE)
mean_pred_probs_df <- data.frame(average_project, mean_pred_probs$fit, mean_pred_probs$se.fit)
mean_pred_probs_df <- mean_pred_probs_df %>% 
  rename(Probability = mean_pred_probs.fit) %>% 
  rename(Publicity = Google_Sentiment)
#32% odds of cancelation
```


### Visualization

```{r prediction_plots}

#With Zeroes
ggplot(pred_probs_df, aes(x = Publicity, y = Probability)) + 
  geom_line() + 
  #geom_ribbon(aes(ymin = Probability - pred_probs.se.fit, ymax = Probability + pred_probs.se.fit), alpha = 0.3) +
  theme_classic() + 
  scale_x_continuous(trans = "reverse") +
  ylab("Probability of Cancellation\n") +
  xlab("\nPublicity Score")

#With no-zeroes
ggplot(pred_probs_df_NoZeroes, aes(x = Publicity, y = Probability)) + 
  geom_line() + 
  #geom_ribbon(aes(ymin = Probability - pred_probs_NoZeroes.se.fit, ymax = Probability + pred_probs_NoZeroes.se.fit), alpha = 0.3) +
  theme_classic() + 
  scale_x_continuous(trans = "reverse") +
  ylab("Probability of Cancellation\n") +
  xlab("\nPublicity Score")

#Using truncated data (best one)
ggplot(pred_probs_trunc_df, aes(x = Publicity, y = Probability)) + 
  geom_line(color = "navy blue") + 
  #geom_ribbon(aes(ymin = Probability - pred_probs_NoZeroes.se.fit, ymax = Probability + pred_probs_NoZeroes.se.fit), alpha = 0.3) +
  theme_classic() + 
  scale_x_continuous(trans = "reverse") +
  ylab("Probability of Cancellation\n") +
  xlab("\nPublicity Score")

#Plot with probabilities of cancelation > 50%
pred_probs_trunc_df_Above50 <- pred_probs_trunc_df %>% 
  filter(Probability > 0.5)

pred_probs_trunc_df_Above50_Low <- pred_probs_trunc_df %>% 
  filter(H_L_W == "Low") %>% 
  filter(Probability > 0.5)

pred_probs_trunc_df_Above50_High <- pred_probs_trunc_df %>% 
  filter(H_L_W == "High") %>% 
  filter(Probability > 0.5)

#Plot with probabilities of cancelation > 70%
pred_probs_trunc_df_Above75 <- pred_probs_trunc_df %>% 
  filter(Probability > 0.75)

mean(pred_probs_trunc_df_Above75_High$Probability)
mean(pred_probs_trunc_df_Above75_Low$Probability)

ggplot(pred_probs_trunc_df_Above75, aes(x = H_L_W, fill = H_L_W)) +
  geom_histogram(width = 0.7, stat = "count") +
  theme_classic() + 
  xlab("Project Location") +
  ylab("Number of Projects\n") +
  scale_y_continuous(expand = c(0,0)) +
  scale_fill_manual(name = "Location",values = c("red","navy blue")) +
  ggtitle("Unfinished Projects with a Probability of Cancellation Greater than 75%\n")
```

```{r distributions_sentiment}

#Canceled and Operating Sentiment

#### USED IN REPORT
ggplot(filter(ventyx_Operating_Canceled, Google_Sentiment != 0), aes(x = Google_Sentiment, fill = OnlyCancel)) + # without 0s
  geom_histogram(bins = 20) +
  xlim(-1.5, 2) +
  theme_classic() +
  scale_y_continuous(expand = c(0,0)) +
  scale_fill_manual(name = "Canceled", values = c("navy blue","red")) +
  geom_vline(aes(xintercept = mean(Google_Sentiment)),col='yellow', size = 2) +
  xlab("Sentiment") + ylab("Number of Projects\n") + ggtitle("Sentiment Distribution - Canceled and Operating Projects")
####

ggplot(ventyx_Operating_Canceled, aes(x = Google_Sentiment, fill = Canceled)) + # with 0s
  geom_histogram(bins = 20) +
  xlim(-1.5, 2) +
  ylim(0, 50) +
  geom_vline(aes(xintercept = mean(Google_Sentiment)),col='red') +
  geom_vline(aes(xintercept = median(Google_Sentiment)),col='green')


#Canceled Sentiment
ggplot(ventyx_Canceled, aes(x = Google_Sentiment)) + # with 0s
  geom_histogram() +
  xlim(-1.5, 2)

ggplot(filter(ventyx_Canceled, Google_Sentiment != 0), aes(x = Google_Sentiment)) + # without 0s
  geom_histogram(bins = 20, fill = "navy blue") +
  geom_vline(aes(xintercept = mean(Google_Sentiment)),col='red', size = 2) +
  xlim(-1.5, 2) +
  theme_classic() +
  scale_y_continuous(expand = c(0,0)) +
  xlab("Sentiment") + ylab("Number of Projects\n") + ggtitle("Sentiment Distribution - Canceled Projects")


#Operating Sentiment
ggplot(ventyx_Operating, aes(x = Google_Sentiment)) + # with 0s
  geom_histogram() +
  xlim(-1.5, 2)
  

ggplot(filter(ventyx_Operating, Google_Sentiment != 0), aes(x = Google_Sentiment)) + # without 0s
  geom_histogram(bins = 20, fill = "navy blue") +
  geom_vline(aes(xintercept = mean(Google_Sentiment)),col='red', size = 2) +
  xlim(-1.5, 2) +
  ylim(0,30) + 
  theme_classic() +
  scale_y_continuous(expand = c(0,0)) +
  xlab("Sentiment") + ylab("Number of Projects\n") + ggtitle("Sentiment Distribution - Operating Projects")

```

```{r distributions_timeline}

ggplot(ventyx_Operating_Canceled, aes(x = TimelineDa, fill = OnlyCancel)) +
  geom_histogram(bins = 20) + 
  theme_classic() +
  scale_y_continuous(expand = c(0,0)) +
  scale_x_continuous(expand = c(0,0), limits = c(0,4500)) + expand_limits(x = 0, y = 0) +
  scale_fill_manual(name = "Canceled", values = c("navy blue","red")) +
  xlab("Timeline (days)") + ylab("Number of Projects\n") + ggtitle("Timeline Distribution - Canceled and Operating Projects")

```

```{r stargazer}

#c("TimelineLength","ViewScore","Sentiment","PopDensity","MedianIncome","Capacity","LowRisk","EnvMemberships","Arkansas","Colorado","Illinois","Indiana","Iowa","Kansas","Minnesota","Missouri","Montanta","Nebraska","NewMexico","NorthDakota","Ohio","Oklahoma","SouthDakota","Wyoming","Sentiment*LowRisk")
#Canceled ~ TimelineDa + View_Score + Google_Sentiment + PopDensity + Household_ + Capacity + H_L_W + members_env + Google_Sentiment*H_L_W + State

stargazer(logistic_reg_all, logistic_reg_all_NoZeroes,
          order=c(7,3),
          type = "text", 
          title = "Logistic Regressions", 
          dep.var.labels = "Log Odds of Cancellation", 
          covariate.labels =c("LowRisk","Sentiment","TimelineLength","ViewScore","PopDensity","MedianIncome","Capacity","EnvMemberships","Sentiment*LowRisk"),
          column.labels = c("NAs As Zeroes","NAs Removed"),
          no.space = FALSE,
          omit=c("State"))
```

```{r gt_table}


reg_DF <- as.data.frame(summary(logistic_reg_all)$coefficients)
reg_DF <- add_rownames(reg_DF, "Variable") %>% 
  select(-"z value") 

reg_DF <- reg_DF[!grepl("State", reg_DF$Variable),]
reg_DF$Variable <- c("Intercept","TimelineLength","ViewScore","Sentiment","PopDensity","MedianIncome","Capacity","LowRisk","EnvMemberships","Sentiment*LowRisk")
colnames(reg_DF) <- c("Variable","Coefficient","SD","P-value")

round_df <- function(x, digits) {
    # round all numeric variables
    # x: data frame 
    # digits: number of digits to round
    numeric_columns <- sapply(x, mode) == 'numeric'
    x[numeric_columns] <-  round(x[numeric_columns], digits)
    x
}

reg_DF_rounded <- round_df(reg_DF, 5)

reg_DF_rounded %>% 
  gt() %>% 
  tab_header(
    title = "Logistic Regression Results", # Add a title
    subtitle = "Predicting Log Odds of Cancellation"# And a subtitle
  ) %>%
  fmt_passthrough( # Not sure about this but it works...
    columns = vars(Variable) # First column: supp (character)
  )

```


```{r examples_for_poster}

#First, find means of all control variables and create vectors with repeated entries
#This is because we want all these variables to be the same across the different example projects, so we can see the effect of sentiment/low-risk
popDensity <- rep(mean(ventyx_Operating_Canceled$PopDensity),4)          #60 thousand
householdMedian <- rep(mean(ventyx_Operating_Canceled$Household_),4)     #$50,525
viewScore <- rep(mean(ventyx_Operating_Canceled$View_Score),4)           #65 Road Points
capacity <- rep(mean(ventyx_Operating_Canceled$Capacity),4)              #117 MW
timelineDays <- rep(mean(ventyx_Operating_Canceled$TimelineDa),4)        #1150 Days
environMemberships <- rep(mean(ventyx_Operating_Canceled$members_env),4) #1348 Members
state <- rep("Texas",4)

#Our key variables for comparison
sentiment <- c(0.1, 0.1, -0.1, -0.1)
location <- c("Low","High","Low","High")

example_df <- data.frame(state, popDensity, householdMedian, capacity, viewScore, timelineDays, environMemberships, sentiment, location)
names(example_df) <- c("State","PopDensity","Household_","Capacity","View_Score","TimelineDa","members_env","Google_Sentiment","H_L_W")

pred_probs_example <- predict(logistic_reg_all, newdata=example_df, type='response', se.fit = TRUE)
pred_probs_example_df <- data.frame(example_df, pred_probs_example$fit, pred_probs_example$se.fit)
```

