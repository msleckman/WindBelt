---
title: "WindBelt Regressions"
author: "WindBelt GP"
date: "January 13, 2018"
output: Word Document
---

```{r setup2, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#### Outline of lexisUni text analysis (checked if done):
 1. Dowload pdf from lexisUni - search terms: "Master Project Name" AND "Project Developer" AND "wind" AND "energy' (if necessary, the state was added in the search term if location was not precise enough. 
 2. Save pdf in sample_pdfs folder x
      Each pdf saved as "fulltext"+ # search results if <10" + "abbreved project developer" "# of results pages in         >10", collapsed with "_" x
2. Create a df with everypdf in each row. my_data x
3. Create df with unested text unested,such that each row is a pdf page.x
4. Merge the pages so that each project with NU articles get a full text compiling all NU articles into 1 cell.
5. Created new df that splits each page by word, sch that every row is a word in the text (unest_tokens where tokens are pdf) x
Note: the same is done the google sentiment csv 
5. Get word count of specific words through group_by()
Conduct sentiment analysis on unique words x 
6. get hits and number of hits of different 'negative words' x
# Following meeting 10/22/18:
1. Unest token by group of words or sentences and conduct sentiment analysis on this.
2. Clean scripts - ID words in pdf that consistently pop up and need to be filtered out.
3. separate headlines from text to ensure we don't have duplicates
4. create csv format (NAME, Developer, State, Sentiment, subjectivity ...)
Other notes: 
*tidytext::tokenize function - every element in list become df. rbind dfs 
 str_count() how many times does a search term come up 
 str_match()
 regex() 
 
```{r load_packages}

### Packages 
library(pdftools)
library(tm)
# install.packages("rlang")
# remove.packages("devtools")
# install.packages("devtools")
library(devtools)
library(dplyr)
library(tidytext)
library(broom)
library(data.table)
# install.packages("colorspace")
library(tidyverse)
library(purrr)
library(googledrive)
library(knitr)
library(readr)
library(stringr)
library(gsubfn)
# install.packages("tidyverse")
# install.packages("tidytext")
# library(tm.plugin.lexisnexis)
# install.packages("gsubfn", "dplyr", "tidytext")
```


```{r connect_to_googledrive}

# The google drive folder id is simply the id in the folder url after the last slash
# So, in this example, the id here is derived from https://drive.google.com/drive/folders/1kZuJF3eS7SIiC8VBeGc6vVNvpBZHLnxg?ogsrc=32

# Create folder in desktop for pdfs. Decided to set on desktop to be compatible with all computers.
# If working on a Bren computer, use this:
NU_PDFS_R <- "H:/Desktop/NU_PDFS_R"

# Alex's Directory:
#NU_PDFS_R <- "C:/Users/airvi/Documents/Bren/GroupProject/NU_PDFS_R"

dir.create(NU_PDFS_R, showWarnings = TRUE) # if directory already exists, will give warning. Ignore.

# Pull all pdfs directly from Google Drive 
NU_PDFs_R_id  <- "1alXSN-uUouUNM2cTHq5OS3LSxVhSDa_v"
NU_PDFs_R_folder <- googledrive::drive_ls(googledrive::as_id(NU_PDFs_R_id))

# function to download all lexisUni pdfs
pdf_downloader <- function(templates_dribble, local_folder){
  # download all pdfs
  
  for (i in 1:nrow(templates_dribble)){
    drive_download(as_id(templates_dribble$id[[i]]), 
                   file.path(local_folder, templates_dribble$name[[i]]),
                   overwrite = F) #check if overwrite is needed here
  }
}

## RUN THE FOLLOWING LINE IF THE PDFS ARE NOTE ALREADY IN A DESKTOP FOLDER ##

pdf_downloader(NU_PDFs_R_folder, NU_PDFS_R)

    #function takes a while, since its pulling all 349 pdfs from googledrive

```



```{r import_data_google_drive}
##### Ventyx ####

##Pulling Ventyx from Google drive to your desktop

CSVs_for_R_script_id  <- "1_G7BGI-Zvz204tz8cs3Zh2tJjT3tmixS"
CSVs_for_R_script_folder <- googledrive::drive_ls(googledrive::as_id(CSVs_for_R_script_id))
desktop_path <- "H:/Desktop/"

pdf_downloader(CSVs_for_R_script_folder, desktop_path)

```

```{r readin_csv}

#For MAC and BREN computers:
ventyx_projects <- read_csv("~/Desktop/Copy_ventyx_converted_01_08_2019_PopDensity_Income_Viewshed_lowhighimpact_doc_names_IncomeMedian.csv")

#For Alex:
#ventyx_projects <- read_csv("C:/Users/airvi/Desktop/ventyx_converted_01_08_2019_PopDensity_Income_Viewshed_lowhighimpact_doc_names_IncomeMedian.csv")

#To view the dataset:
#View(ventyx_projects)

#To change the 0-1 definition to Low-High
ventyx_df <- mutate(ventyx_projects, H_L_W = ifelse(lr_tif>0, "Low", "High"))

#Remove whitespace for name matching later one
ventyx_df$ProjectName <- gsub(" ", "", ventyx_df$ProjectName, fixed = TRUE)
ventyx_df$document <- gsub(" ", "", ventyx_df$document, fixed = TRUE)

#### Google News ######
#Before running this code, make sure you have the Google scraping dataset saved on your desktop. The file path that the "read_csv" function accesses should be where the file is located on your desktop. 

#For MAC and BREN computers:
google_df <- read_csv("~/Desktop/Copy_google_scraping_01-08-19.csv")

#For Alex:
#google_df <- read_csv("C:/Users/airvi/Desktop/google_scraping_01-08-19.csv")

#To view the dataset:
#View(google_df)


```

# Pull all pdf text from NU_PDFs_R 
```{r nexus_directory_setup}

#For MAC and BREN computers:
pdf_directory <- '~/Desktop/NU_PDFS_R'

#For Alex:
#pdf_directory <- "C:/Users/airvi/Documents/Bren/GroupProject/NU_PDFS_R"

#For Delaney:
#pdf_directory <- "D:/Desktop/All_LexisUni_PDFs"
#pdf_directory <- "~/Desktop/All_LexisUni_PDFs"

#Listing all PDFs: should be 349 PDFs
pdfs <- paste(pdf_directory, "/", list.files(pdf_directory, pattern = "*.pdf", ignore.case = T), sep = "")

#PDF names
pdfs_names <- list.files(pdf_directory, pattern = "*.pdf", ignore.case = T)

#PDF text
pdfs_text <- purrr::map(pdfs, pdftools::pdf_text)

  #Takes a minute...
  #Expect 9 'PDF error' and ignore. Text normally still processed
```

#### Create initial dataframes that include document name and text
```{r dataframe_creation}

##### NU ####
#This combines the pdfs_names and pdfs_texts variables from the previous code chunk, into a single dataframe
#Each row is a PDF doc name with the full pdf text. Note: in the text column, each row is an element of a list
projects_NU <- data_frame(document = pdfs_names, text = pdfs_text)

#### Google ####
#This creates a data frame of just the project name and full text for each Google News article
#Each row is the project name with the full article text
projects_google <- data_frame(document = google_df$ProjectName, text = google_df$FullText)

```

#### Split text by page (only NU)
```{r page_split_aggregate}

#Dataset with each page in one row
project_pdfpages_NU <- projects_NU %>% 
  unnest() # splits pdf text by page and removes list format ( c("")) since each element is now its own row.

#Collapse pages so that every row under text is the full pdf. Chose to indicate page separation by (/page)
project_pdfs_full_text <- project_pdfpages_NU %>%
  group_by(document) %>%
  summarise(text = paste(text, collapse = " (/p) "))
    # note: if you write this to a csv, the next will go the next line and won't look like a clean csv (i.e. Alex's google news csv). TBc.

write_csv(project_pdfs_full_text, "TextAnalysis/NexisUni/project_pdfs_full_text.csv")

```

#### Split text by word (unnest_tokens())
```{r word_split}
#### NU ####
#Dataset with each word in a row associated with its pdf source
#Also filters out unwanted words

projects_words_NU <- project_pdfs_full_text %>%
  tidytext::unnest_tokens(output = word, input = text, token = 
                          "words", to_lower = T) %>%   # important to put all words to lower because it AFINN dictionnary has words only in lower case format     
  filter(!word %in% c("lexis",
                      "nexis", 
                      "Uni",
                      "about lexisnexis",
                      "Privacy Policy",
                      "Terms & Conditions", 
                      "Copyright ? 2018 LexisNexis",
                      " | ",  
                      "@", 
                      "lexisnexis", "(/p)"))

#head(projects_words_NU)
length(unique(projects_words_NU$document)) # results: still 349 projects here after word parsing
# 
# projects_words_NU <- projects_NU %>% 
#   unnest() %>% 
#   tidytext::unnest_tokens(output = word, input = text, token = 
#                           "words", to_lower = T) %>%      
#   filter(!word %in% c("lexis",
#                       "nexis", 
#                       "Uni",
#                       "about lexisnexis",
#                       "Privacy Policy",
#                       "Terms & Conditions", 
#                       "Copyright Â© 2018 LexisNexis",
#                       " | ",  
#                       "@", 
#                       "lexisnexis")) 

# %>% gsub("[^A-Za-z0-9,;._-]","")

#projects_pdfnest_NU <- projects_pdftext %>% 
#  unnest() %>% 
#  tidytext::unnest_tokens(output = ngrams, input = text, token = "ngrams", n = 5, to_lower = T)

# note: unnest_tokens() splits text by respective element (ie word, phrase, ...) word is default


#### Google #####
#Dataset with each word in a row associated with its project source 
projects_words_google <- projects_google %>% 
  tidytext::unnest_tokens(output = word, input = text, token = "words", to_lower = F)
head(projects_words_google)

```

#### Group words by pdf/project and summarize by frequency
```{r group_by}

#### NU ####
projects_words_count_NU <- projects_words_NU %>%
  group_by(document, word) %>% 
  summarise(count = n())
#View(projects_pdfwords_count)
#Counts the number of time a specific words is found in the pdf page

length(unique(projects_words_count_NU$document)) # results: there are still 349 projects here 

# projects_pdfnest_count <- projects_pdfnest_NU %>%
#   group_by(document, ngrams) %>% 
#   summarise(count = n())
#View(projects_pdfnest_count)
#add new count column with most freq. words


#### Google ####
projects_words_count_google <- projects_words_google %>%
  group_by(document, word) %>% 
  summarise(count = n())
#View(projects_pdfwords_count)
#Counts the number of time a specific words is found in the article
```

#### Sentiment dictionaries
```{r sentiment_dictionaries}
# Using 'afinn' vs. 'nrc sentiment tests.

get_sentiments("afinn") # associates word with a sentiment score
#afinn scores/ranks from -5 to +5 for positive or negative sentiment.

get_sentiments("nrc") # associated word with another sentiment feeling word

# View(get_sentiments("afinn"))
# View(get_sentiments("nrc"))

# We want scores not categorized words, so we will use AFINN for now.
```

#### Bind Sentiments
```{r bind_sentiment}
#### NU ####
projects_score_bind_NU <-projects_words_count_NU %>% 
  left_join(get_sentiments("afinn"), by = "word") %>% 
  # filter(score != "NA") %>% 
  filter(!is.na(score)) 
# Note: Many of the scores per words are NA simply because that word does not exist. 
  
unique(projects_score_bind_NU$document)

# look at which projects have NA all NA's as words
projects_score_bind_NU_NA <-projects_words_count_NU %>% 
  left_join(get_sentiments("afinn"), by = "word") %>% 
  filter(is.na(score)) 


length(unique(projects_score_bind_NU$document)) # 349
### general examples

## projects with few words
projects_score_bind_NU %>% 
  filter(document == "Allendorf_NorthernAlternativeEnergy_1(2).PDF")

projects_score_bind_NU %>% 
  filter(document == "GreatPathfinderWind_ApexCleanEnergy_1(4).PDF")

## more words
projects_score_bind_NU %>% 
  filter(document == "ArranzWindTuxedo_Energos_1.PDF")

#### Google ####
projects_score_bind_google <- projects_words_count_google %>% 
  left_join(get_sentiments("afinn"), by = "word") %>% 
  filter(score != "NA")

#View(projects_score_bind)

# Note: Many of the scores per words are NA simply because that word does not exist. 
```

#### Determine Project Scores
```{r projectscores}

#To determine the total score for each document (NU) or project (Google)

# View(ventyx_df)

##### NU ######
total_sentiment_with_stats_NU <- projects_score_bind_NU %>% 
  #filter(score !="NA") %>% 
  group_by(document) %>% 
  summarise(totals = weighted.mean(score, w = count),
            standard_dev = sd(score), 
            variance = var(score))

total_sentiment_NU <- projects_score_bind_NU %>% 
  #filter(score !="NA") %>% 
  group_by(document) %>% 
  summarise(totals = mean(score))
  
#View(total_sentiment_with_stats_NU)
# View(total_sentiment_NU)
length(unique(total_sentiment_NU$document)) # --> 372

#### Google #####
total_sentiment_with_stats_google <- projects_score_bind_google %>% 
  #filter(score !="NA") %>% 
  group_by(document) %>% 
  summarise(totals = weighted.mean(score, w = count),
            standard_dev = sd(score), 
            variance = var(score)
            )

total_sentiment_google <- projects_score_bind_google %>% 
  #filter(score !="NA") %>% 
  group_by(document) %>% 
  summarise(totals = mean(score))
  

```

####Combine sentiment scores with original data
```{r combinewithsen}

#### NU ####

#Convert to data frame
total_sentiment_NU_df = as.data.frame(total_sentiment_NU)
# View(total_sentiment_NU_df)
# View(ventyx_df)
# To ensure the project name from Ventyx matches properly with the document name in total_sentiment_NU

# Remove white space in document names in ventyx and total_sentiment_NU_df


# mutate new column with project names
# total_sentiment_NU_df <- total_sentiment_NU_df %>%
#   mutate(ProjectName = sub("\\_.*$", "", document)) # extract words before first '_' in document name

#NU
total_sentiment_NU_df$document <- gsub(" ", "", total_sentiment_NU_df$document)

#Ventyx 
# ventyx_df$document <- gsub(" ", "", ventyx_df$document)
  
# View(ventyx_df)
# View(total_sentiment_NU_df)

#Merge with original data (ventyx_df)
# 
# View(ventyx_df)
# View(total_sentiment_NU_df)
# 
# a <- intersect(ventyx_df$document, total_sentiment_NU_df$document) 
# setdiff(total_sentiment_NU_df$document, a)

ventyx_df[ventyx_df$ProjectName == "71RWind",]$document = "71RWind_71Ranch_1(1).PDF"
ventyx_df[ventyx_df$ProjectName == "CHillWindFarm",]$document = "CHill_ExergyIntegratedSystems_1.PDF"
ventyx_df[ventyx_df$ProjectName == "AdamsProject",]$document = "AdamsProject_MidAmerican_1(2).PDF"
ventyx_df[ventyx_df$ProjectName == "LJTrust",]$document = "LJTrust_LJTrust_1(1).PDF"
ventyx_df[ventyx_df$ProjectName == "MinonkWindFarm",]$document = "MinonkWindFarm_MinonkWindFarm_1.PDF"

  
ventyx_df_NU_Sen <- merge(ventyx_df, total_sentiment_NU_df, by = "document", all = TRUE)
# ventyx_df_NU_Sen <- merge(ventyx_df, total_sentiment_NU_df, by = "document", all = TRUE) #old

#Rename "totals" column

ventyx_df_NU_Sen <- rename(ventyx_df_NU_Sen, NU_Sentiment=totals)

# View(ventyx_df_NU_Sen)
length(unique(ventyx_df_NU_Sen$document)) # includes NA as a unique(document) NU scores since many projects do not have sentiment scores
length(unique(total_sentiment_NU_df$document)) # excludes NA as a document


#### Google #####
#Convert to data frame
total_sentiment_google_df = as.data.frame(total_sentiment_google)
#Remove white space in order to perform merge (google data has extra white spaces)
total_sentiment_google_df$document <- gsub(" ", "", total_sentiment_google_df$document, fixed = TRUE)


#Merge with original data
ventyx_df_google_Sen <- merge(ventyx_df, total_sentiment_google_df, by.x ="ProjectName", by.y = "document", all = TRUE)
#Rename "totals" columns
ventyx_df_google_Sen <- rename(ventyx_df_google_Sen, Google_Sentiment=totals)


#### Combined ####
ventyx_df_NU_google_Sen <- merge(ventyx_df_NU_Sen, total_sentiment_google_df, by.x = "ProjectName", by.y = "document", all = TRUE)

ventyx_df_NU_google_Sen <- ventyx_df_NU_google_Sen %>% 
  rename(Google_Sentiment=totals) %>% 
  mutate(NU_Sentiment = ifelse(is.na(NU_Sentiment),0,NU_Sentiment)) %>% 
  mutate(Google_Sentiment = ifelse(is.na(Google_Sentiment),0,Google_Sentiment))


ventyx_df_NU_google_Sen <- ventyx_df_NU_google_Sen %>% 
  filter(Capacity >= 0) %>% 
  filter(TimelineDays > 0)

length(unique(ventyx_df_NU_google_Sen$ProjectName))

### For descriptive stats

#write.csv(ventyx_df_NU_google_Sen, "~/Desktop/ventyx_df_NU_google_Sen_01.18.19.csv")
write_csv(total_sentiment_google_df, "TextAnalysis/googlenews/total_sentiment_google_df.csv")
write_csv(total_sentiment_NU_df, "TextAnalysis/NexisUni/total_sentiment_NU_df.csv")

```

####Convert sentiment scores to positive or negative in new column (optional)
```{r pos_or_neg_sen}

ventyx_df_NU_Sen <- mutate(ventyx_df_NU_Sen, Sign_NU = ifelse(NU_Sentiment>0, "Positive", "Negative"))
ventyx_df_google_Sen <- mutate(ventyx_df_google_Sen, Sign_Google = ifelse(Google_Sentiment>0, "Positive", "Negative"))
ventyx_df_NU_google_Sen <- ventyx_df_NU_google_Sen %>% 
  mutate(Sign_Google = ifelse(Google_Sentiment>=0, "Positive", "Negative")) %>% 
  mutate(Sign_NU = ifelse(NU_Sentiment>=0, "Positive", "Negative"))


```

####Add environmental memberships data
```{r env_members}
#For Alex
#zip_conversion <- read_csv("C:/Users/airvi/Documents/Bren/GroupProject/zip_code_database.csv")

#For Bren computers
zip_conversion <- read_csv("~/Desktop/copy_zip_code_database.csv")

#For Alex
#memberships <- read_csv("C:/Users/airvi/Documents/Bren/GroupProject/membership_data_combined.csv")

#For Bren computers
memberships <- read_csv("~/Desktop/Copy_membership_data_combined.csv")

#Make sure all zip codes are 5 digits
memberships$zip <- sprintf("%05d", memberships$zip)

#Merge memberships data with county
zip_county_merge <- merge(zip_conversion, memberships, by = "zip")

#Remove the word county, parish, and municipality, then trim any trailing whitespace
zip_county_merge$county <- gsub("County","",zip_county_merge$county)
zip_county_merge$county <- gsub("Parish","",zip_county_merge$county)
zip_county_merge$county <- gsub("Municipality","",zip_county_merge$county)
zip_county_merge$county <- trimws(zip_county_merge$county, "r")

#Group by county and sum all of the members for environmental organizations
zip_county_merge_aggregate <- zip_county_merge %>% 
  group_by(county) %>% 
  summarise(members_env = sum(members_nrdc,members_nwf,members_tnc))

#Merge with ventyx dataset and set NA membership values to zero
ventyx_df_NU_google_Sen <- merge(ventyx_df_NU_google_Sen, zip_county_merge_aggregate, by.x = "County", by.y = "county", all.x = TRUE)
ventyx_df_NU_google_Sen$members_env[is.na(ventyx_df_NU_google_Sen$members_env)] <- 0


names(ventyx_df_NU_google_Sen)
length(ventyx_df_NU_google_Sen$ProjectName)

```


####Prepare the relevant columns for regression
```{r organize_for_regressions}
#na.omit(ventyx_df_NU_Sen$Household_MeanIncome)
#na.omit(ventyx_df_NU_Sen$PopDensity_mi)
#na.omit(ventyx_df_NU_Sen$lr_tif)
#na.omit(ventyx_df_NU_Sen$View_Score)

#### NU ####
ventyx_df_NU_Sen$H_L_W <- as.factor(ventyx_df_NU_Sen$H_L_W)

ventyx_df_NU_Sen$PopDensity_mi <- as.numeric(ventyx_df_NU_Sen$PopDensity_mi)

ventyx_df_NU_Sen$Household_MedianIncome <- as.numeric(ventyx_df_NU_Sen$Household_MedianIncome)

ventyx_df_NU_Sen$View_Score <- as.numeric(ventyx_df_NU_Sen$View_Score)

ventyx_df_NU_Sen$H_L_W <- relevel(ventyx_df_NU_Sen$H_L_W, ref = "High")

ventyx_df_NU_Sen$Sign_NU <- as.factor(ventyx_df_NU_Sen$Sign_NU)

#str(ventyx_df_NU_Sen)

#summary(ventyx_df_NU_Sen)

#### Google ####
ventyx_df_google_Sen$H_L_W <- as.factor(ventyx_df_google_Sen$H_L_W)

ventyx_df_google_Sen$PopDensity_mi <- as.numeric(ventyx_df_google_Sen$PopDensity_mi)

ventyx_df_google_Sen$Household_MedianIncome <- as.numeric(ventyx_df_google_Sen$Household_MedianIncome)

ventyx_df_google_Sen$View_Score <- as.numeric(ventyx_df_google_Sen$View_Score)

ventyx_df_google_Sen$H_L_W <- relevel(ventyx_df_google_Sen$H_L_W, ref = "High")

ventyx_df_google_Sen$Sign_Google <- as.factor(ventyx_df_google_Sen$Sign_Google)

#str(ventyx_df_google_Sen)

#summary(ventyx_df_google_Sen)

#### Combined #### 


ventyx_df_NU_google_Sen$H_L_W <- as.factor(ventyx_df_NU_google_Sen$H_L_W)

ventyx_df_NU_google_Sen$PopDensity_mi <- as.numeric(ventyx_df_NU_google_Sen$PopDensity_mi)

ventyx_df_NU_google_Sen$Household_MedianIncome <- as.numeric(ventyx_df_NU_google_Sen$Household_MedianIncome)

ventyx_df_NU_google_Sen$View_Score <- as.numeric(ventyx_df_NU_google_Sen$View_Score)

ventyx_df_NU_google_Sen$H_L_W <- relevel(ventyx_df_NU_google_Sen$H_L_W, ref = "High")

ventyx_df_NU_google_Sen$Sign_Google <- as.factor(ventyx_df_NU_google_Sen$Sign_Google)

ventyx_df_NU_google_Sen$Sign_NU <- as.factor(ventyx_df_NU_google_Sen$Sign_NU)

str(ventyx_df_NU_google_Sen)

summary(ventyx_df_NU_google_Sen)

### adding normalization to df 

normalize <- function(x){
  norm_x <- (x - mean(x, na.rm = T))/sd(x, na.rm =T)
  return(norm_x)
  }

ventyx_df_NU_google_Sen_norm <- ventyx_df_NU_google_Sen %>% 
  select(-starts_with("X")) %>% 
  mutate(Capacity_2 = normalize(Capacity),
         View_Score_2 = normalize(View_Score),
         Google_Sentiment_2 = normalize(Google_Sentiment),
         NU_Sentiment_2 = normalize(NU_Sentiment),
         PopDensity_mi_2 = normalize(PopDensity_mi),
         Household_MedianIncome_2 = normalize(Household_MedianIncome),
         members_env_2 = normalize(members_env)) 
  
length(ventyx_df_NU_google_Sen$ProjectName)

```

####Run Regressions


```{r prep}

#install.packages
require(broom)
require(dplyr)
# install.packages("dotwhisker")
library(dotwhisker)

install.packages("jtools")
library(jtools)

renamed_variables <- c(View_Score = "View Score",
                        Google_Sentiment = "Google News Publicity score",
                        NU_Sentiment = "Nexis Uni Publicity score",
                        PopDensity_mi = "Population Density per m2",
                        Household_MedianIncome = "Household Median Income",
                        Capacity = "Capacity",
                        H_L_WLow = "High vs. low impact area"
                        # H_L_W*NU_Sentiment = "site vs. sentiment interaction"
                       )

```

### NA Sentiment scores coded as '0' - w/ interaction term
#### NU only 
```{r R7}

####7. All variables included -- using only NU scores and an interaction between high-low and NU sentiment. N/A scores coded as "0".
reg_TLD_ZEROES_VS_NSen_PopDen_MedianIncome_Cap_HL_State_Inter <- lm(TimelineDays ~ View_Score + NU_Sentiment + PopDensity_mi + Household_MedianIncome + Capacity + H_L_W + State + H_L_W*NU_Sentiment, data=ventyx_df_NU_google_Sen)


summ(reg_TLD_ZEROES_VS_NSen_PopDen_MedianIncome_Cap_HL_State_Inter)

# 896 observations - ventyx_df_NU_google_Sen has 902 but 6 were removed as they were missing values (one more of the variables had an NA)


reg_TLD_ZEROES_VS_NSen_PopDen_MedianIncome_Cap_HL_State_Inter
summary(reg_TLD_ZEROES_VS_NSen_PopDen_MedianIncome_Cap_HL_State_Inter)
# dwplot(reg_TLD_ZEROES_VS_NSen_PopDen_MedianIncome_Cap_HL_State_Inter)+
#   theme_bw()+
#   labs(x = "timeline")+

str(reg_TLD_ZEROES_VS_NSen_PopDen_MedianIncome_Cap_HL_State_Inter)
summary(reg_TLD_ZEROES_VS_NSen_PopDen_MedianIncome_Cap_HL_State_Inter)

dwplot(reg_TLD_ZEROES_VS_NSen_PopDen_MedianIncome_Cap_HL_State_Inter,
       dot_args = list(colour = "firebrick"), whisker_args = list(colour = "firebrick"))+
  ggplot2::geom_vline(xintercept = 0, colour = "black", linetype = 2)+
    theme_bw()+
  labs(x = "timeline (days)")

regression2 <- tidy(reg_TLD_ZEROES_VS_NSen_PopDen_MedianIncome_Cap_HL_State_Inter)

dwplot(regression2[!grepl("^State*", regression2$term),] %>%  relabel_predictors(renamed_variables),
   dot_args = list(colour = "firebrick"),
   whisker_args = list(colour = "firebrick"))+
  labs(x = "Timeline (days)", y = "")+
  ggplot2::geom_vline(xintercept = 0, colour = "black", linetype = 2)+
  theme_classic()+
  ggtitle("Regression output: all variables, using only NU scores\n and interaction term. N/A scores coded as 0:")+
  theme(legend.background = element_rect(colour="grey80"))
  
#Results: significance found in capacity (0.07323)
#NU_Sentiment: 0.505
#H_L_W: 0.643
#Interaction: 0.26455

# 01/20/18 - With added text from NU
#Results: significance found in capacity (0.05121)
#NU_Sentiment: 0.539
#H_L_W: 0.954
#Interaction: 0.56229



```

##### Google only
```{r R9}

####9. All variables included -- using only Google scores and an interaction between high-low and Google sentiment. N/A scores coded as "0"
reg_TLD_ZEROES_VS_GSen_PopDen_MedianIncome_Cap_HL_State_Inter <- lm(TimelineDays ~ View_Score + Google_Sentiment + PopDensity_mi + Household_MedianIncome + Capacity + H_L_W + State + H_L_W*Google_Sentiment, data=ventyx_df_NU_google_Sen)

names(ventyx_df_NU_google_Sen)

summary(reg_TLD_ZEROES_VS_GSen_PopDen_MedianIncome_Cap_HL_State_Inter)
summ(reg_TLD_ZEROES_VS_GSen_PopDen_MedianIncome_Cap_HL_State_Inter)
  
## note: wen env.members is present, get a google_sentiment p value=0.01029 
  ## when env.members is NOT present, get a google_sentiment p value=0.01023 
  ## coef. for google.sentiment stays same 

## everything
dwplot(reg_TLD_ZEROES_VS_GSen_PopDen_MedianIncome_Cap_HL_State_Inter, dot_args = list(colour = "firebrick"), whisker_args = list(colour = "firebrick"))+
    theme_bw()+
  ggplot2::geom_vline(xintercept = 0, colour = "black", linetype = 2)+
  labs(x = "Timeline (days)")

regression3 <- tidy(reg_TLD_ZEROES_VS_GSen_PopDen_MedianIncome_Cap_HL_State_Inter)

# Main variables
dwplot(regression3[!grepl("^State*", regression3$term),] %>%  
   relabel_predictors(renamed_variables), 
   dot_args = list(colour = "darkgreen"),
   whisker_args = list(colour = "darkgreen")
   )+
  labs(x = "Timeline (days)", y = "")+
  ggplot2::geom_vline(xintercept = 0, colour = "black", linetype = 2)+
  theme_classic()+
  # ggtitle("Regression output: all variables, using only Google scores \n and interaction term. N/A scores coded as 0:")+
  theme(legend.background = element_rect(colour="grey80"))


#Results: significance found in Google sentiment (0.01327) and capacity (0.09445)
#H_L_W: 0.63589
#Interaction: 0.204
####
#  With added text and removal of capacity/timelinedays  01/21/18
#Results: significance found in Google sentiment (0.01023) and capacity (0.06793)
#H_L_W: 0.54475
#Interaction: 0.18378
####



```

```{r separatedplots}


library(RColorBrewer)


View(regression1)

length(ventyx_df_google_Sen)

dwplot(regression3[!grepl("^State*|View_Score|PopDensity_mi|Household_MedianIncome|Capacity|NU_Sentiment|NU_Sentiment:H_L_WLow", regression3$term),]%>% 
 relabel_predictors(renamed_variables), 
   dot_args = list(colour = "forestgreen", lwd = 1.5),
   whisker_args = list(colour = "forestgreen", lwd = 0.8)
   )+
  labs(x = "Timeline (days)", y = "")+
  ggplot2::geom_vline(xintercept = 0, colour = "grey80", linetype = 2)+
  theme_classic()+
  # ggtitle("Regression output: all variables including NU and Google \n scores, interaction terms. N/A scores coded as 0:")+
  theme(legend.background = element_rect(colour="black"),
        axis.text.y = element_text(size = 11, hjust=1),
        axis.text.x = element_text(size = 11, hjust=1),
        axis.title.x.bottom = element_text(size=13, vjust = -1))

View(regression3)

dwplot(regression3[!grepl("^State*|H_L_WLow|Google_Sentiment|Google_Sentiment:H_L_WLow", regression3$term),] %>% 
   relabel_predictors(renamed_variables), 
   dot_args = list(colour = "forestgreen", lwd = 1.5),
   whisker_args = list(colour = "forestgreen", lwd = 0.75)
   )+
  labs(x = "Timeline (days)", y = "")+
  ggplot2::geom_vline(xintercept = 0, colour = "grey80", linetype = 2)+
  theme_classic()+
  # ggtitle("Regression output: all variables including NU and Google \n scores, interaction terms. N/A scores coded as 0:")+
  theme(legend.background = element_rect(colour="black"),
        axis.text.y = element_text(size = 11, hjust=1),
        axis.text.x = element_text(size = 11, hjust=1),
        axis.title.x.bottom = element_text(size=13, vjust = -1))
```


#### Google and NU 
```{r R10}
####10. All variables included -- using both NU and Google scores and an interaction between the high-low variable and both NU and Google sentiment. N/A scores coded as "0"
reg_TLD_ZEROES_VS_GNSen_PopDen_MedianIncome_Cap_HL_State_Inter <- lm(TimelineDays ~ View_Score + Google_Sentiment + NU_Sentiment + PopDensity_mi + Household_MedianIncome + Capacity + H_L_W + State + H_L_W*Google_Sentiment + H_L_W*NU_Sentiment, data=ventyx_df_NU_google_Sen)

summary(reg_TLD_ZEROES_VS_GNSen_PopDen_MedianIncome_Cap_HL_State_Inter)

dwplot(reg_TLD_ZEROES_VS_GNSen_PopDen_MedianIncome_Cap_HL_State_Inter)+
  theme_bw()+
  labs(x = "timeline")


# reg_TLD_ZEROES_VS_GNSen_PopDen_MedianIncome_Cap_HL_State$terms


# everything 
dwplot(reg_TLD_ZEROES_VS_GNSen_PopDen_MedianIncome_Cap_HL_State_Inter, dot_args = list(colour = "firebrick"), whisker_args = list(colour = "firebrick", lwd=0.5))+
  ggplot2::geom_vline(xintercept = 0, colour = "black", linetype = 2)+
    theme_classic()+
  labs(x = "timeline(days)")+
    ggtitle("Regression output: all variables, including NU and Google \n scores. N/A scores coded as 0:")+
  theme(legend.background = element_rect(colour="grey80"))


regression1 <- tidy(reg_TLD_ZEROES_VS_GNSen_PopDen_MedianIncome_Cap_HL_State_Inter)

dwplot(regression1[!grepl("^State*", regression1$term),] %>% 
   relabel_predictors(renamed_variables), 
   dot_args = list(colour = "firebrick"),
   whisker_args = list(colour = "firebrick", lwd = 0.5)
   )+
  labs(x = "Timeline (days)", y = "")+
  ggplot2::geom_vline(xintercept = 0, colour = "black", linetype = 2)+
  theme_classic()+
  ggtitle("Regression output: all variables including NU and Google \n scores, interaction terms. N/A scores coded as 0:")+
  theme(legend.background = element_rect(colour="grey80"))



#Results: significance found in Google sentiment (0.0067)
#NU_Sentiment: 0.196
#H_L_W: 0.804
#Interaction Google: 0.10846
#Interaction NU: 0.15778
####
# 01/21/18 - difference in results with changes to ventyx df and text from NU:
#Results: significance found in Google sentiment (0.00522)
#NU_Sentiment: 0.207
#H_L_W: 0.88696
#Interaction Google: 0.12861
#Interaction NU: 0.38246
####

```


```{r combined_with_interaction term}


R1 <- reg_TLD_ZEROES_VS_GNSen_PopDen_MedianIncome_Cap_HL_State_Inter
R2 <- reg_TLD_ZEROES_VS_NSen_PopDen_MedianIncome_Cap_HL_State_Inter
R3 <- reg_TLD_ZEROES_VS_GSen_PopDen_MedianIncome_Cap_HL_State_Inter

# general_everything plot

# unique(regression1$term)

# dwplot(list(tidy(R1),tidy(R2),tidy(R3), conf.int = FALSE)
#        # dot_args = list(colour = "firebrick"), whisker_args = list(colour = "firebrick")
#        )+
#     theme_bw()+
#   labs(x = "timeline")


### does not work unless subset by regression 
regression1_1 <- tidy(R1)  %>% 
  filter(!grepl('^State*', term)) %>% 
  mutate(model = "regression1_G+NU")

regression2_1 <- tidy(R2)  %>% 
  filter(!grepl('^State*', term)) %>% 
  mutate(model = "regression2_NU")

regression3_1 <- tidy(R3)  %>% 
  filter(!grepl('^State*', term)) %>% 
  mutate(model = "regression3_G")

combined_reg <-  bind_rows(regression1_1, regression2_1, regression3_1)
# View(combined_reg)


dwplot(combined_reg %>% 
   relabel_predictors(renamed_variables), 
   dot_args = list(aes(shape = model)),
   whisker_args = list(aes(colour = model))
   )+
  labs(x = "Timeline (days)", y = "")+
  ggplot2::geom_vline(xintercept = 0, colour = "grey80", linetype = 2)+
  theme_classic()+
  ggtitle("Combined regression outputs with interaction term:\n N/A scores coded as 0:\n")+
  theme(plot.title = element_text(face="bold"),
          legend.position = c(0.7, 0.77),
          legend.justification = c(0, 0),
          legend.background = element_rect(colour = "grey60"),
          legend.title.align = .5)
  



```

### NA Sentiment scores coded as '0' - No interaction term

#### NU and Google 
```{r R5}

####5. All variables included -- using both NU and Google scores. N/A scores coded as "0":

reg_TLD_ZEROES_VS_GNSen_PopDen_MedianIncome_Cap_HL_State <- lm(TimelineDays ~ View_Score + Google_Sentiment + NU_Sentiment + PopDensity_mi + Household_MedianIncome + Capacity + H_L_W + State, data=ventyx_df_NU_google_Sen)

summary(reg_TLD_ZEROES_VS_GNSen_PopDen_MedianIncome_Cap_HL_State)

# reg_TLD_ZEROES_VS_GNSen_PopDen_MedianIncome_Cap_HL_State$terms

renamed_variables <- c(View_Score = "View Score",
                        Google_Sentiment = "Google News Publicity score",
                        NU_Sentiment = "Nexis Uni Publicity score",
                        PopDensity_mi = "Population Density per m2",
                        Household_MedianIncome = "Household Median Income",
                        Capacity = "Capacity",
                        H_L_WLow = "High vs. low impact area")


# everything 
dwplot(tidy(reg_TLD_ZEROES_VS_GNSen_PopDen_MedianIncome_Cap_HL_State), dot_args = list(colour = "firebrick"), whisker_args = list(colour = "firebrick", lwd=0.5))+
  ggplot2::geom_vline(xintercept = 0, colour = "black", linetype = 2)+
    theme_classic()+
  labs(x = "timeline")+
    ggtitle("Regression output: all variables, including NU and Google scores. N/A \n scores coded as 0:")+
  theme(legend.background = element_rect(colour="grey80"))


regression1 <- tidy(reg_TLD_ZEROES_VS_GNSen_PopDen_MedianIncome_Cap_HL_State)

dwplot(regression1[!grepl("^State*", regression1$term),] %>% 
   relabel_predictors(renamed_variables), 
   dot_args = list(colour = "firebrick"),
   whisker_args = list(colour = "firebrick", lwd = 0.5)
   )+
  labs(x = "Timeline (days)", y = "")+
  ggplot2::geom_vline(xintercept = 0, colour = "black", linetype = 2)+
  theme_classic()+
  ggtitle("Regression output: all variables, including NU and Google scores. N/A \n scores coded as 0:")+
  theme(legend.background = element_rect(colour="grey80"))
  

#Results: significance found in Google sentiment (0.02867) and capacity (0.09094)
#H_L_W p-value: 0.91039
#NU_Sentiment: 0.57313
####
## 01/20/18 - MS: with additional text, got more significance on some values it seems 

```

#### NU only
```{r R6}
####6. All variables included -- using only NU scores. N/A scores coded as "0".
reg_TLD_ZEROES_VS_NSen_PopDen_MedianIncome_Cap_HL_State <- lm(TimelineDays ~ View_Score + NU_Sentiment + PopDensity_mi + Household_MedianIncome + Capacity + H_L_W + State, data=ventyx_df_NU_google_Sen)

summary(reg_TLD_ZEROES_VS_NSen_PopDen_MedianIncome_Cap_HL_State)

dwplot(reg_TLD_ZEROES_VS_NSen_PopDen_MedianIncome_Cap_HL_State, dot_args = list(colour = "firebrick"), whisker_args = list(colour = "firebrick"))+
  ggplot2::geom_vline(xintercept = 0, colour = "black", linetype = 2)+
    theme_bw()+
  labs(x = "timeline")

regression2 <- tidy(reg_TLD_ZEROES_VS_NSen_PopDen_MedianIncome_Cap_HL_State)

dwplot(regression2[!grepl("^State*", regression2$term),] %>% 
   relabel_predictors(renamed_variables), 
   dot_args = list(colour = "firebrick"),
   whisker_args = list(colour = "firebrick")
   )+
  labs(x = "Timeline (days)", y = "")+
  ggplot2::geom_vline(xintercept = 0, colour = "black", linetype = 2)+
  theme_classic()+
  ggtitle("Regression output: all variables, using only NU scores. N/A \n scores coded as 0:")+
  theme(legend.background = element_rect(colour="grey80"))
  

#Results: significance found in capacity (0.07004)
#NU_Sentiment: 0.986
#H_L_W: 0.856

```

```{r R7}
####7. All variables included -- using only NU scores and an interaction between high-low and NU sentiment. N/A scores coded as "0".
reg_TLD_ZEROES_VS_NSen_PopDen_MedianIncome_Cap_HL_State_Inter <- lm(TimelineDays ~ View_Score + NU_Sentiment + PopDensity_mi + Household_MedianIncome + Capacity + H_L_W + State + H_L_W*NU_Sentiment, data=ventyx_df_NU_google_Sen)

## 01/20/18 - MS: with additional text, got more significance on values 
#Results: significance found in capacity (0.04916)
#NU_Sentiment: 0.73846
#H_L_W: 0.73898

```

#### Google only
```{r R8}

####8. All variables included -- using only Google scores. N/A scores coded as "0"
reg_TLD_ZEROES_VS_GSen_PopDen_MedianIncome_Cap_HL_State <- lm(TimelineDays ~ View_Score + Google_Sentiment + PopDensity_mi + Household_MedianIncome + Capacity + H_L_W + State, data=ventyx_df_NU_google_Sen)

summary(reg_TLD_ZEROES_VS_GSen_PopDen_MedianIncome_Cap_HL_State)

## everything
dwplot(tidy(reg_TLD_ZEROES_VS_GSen_PopDen_MedianIncome_Cap_HL_State), dot_args = list(colour = "firebrick"), whisker_args = list(colour = "firebrick"))+
    theme_bw()+
  ggplot2::geom_vline(xintercept = 0, colour = "black", linetype = 2)+
  labs(x = "Timeline (days)")

regression3 <- tidy(reg_TLD_ZEROES_VS_GSen_PopDen_MedianIncome_Cap_HL_State) %>% 
  mutate()

# Main variables
dwplot(regression3[!grepl("^State*", regression3$term),] %>%  
   relabel_predictors(renamed_variables), 
   dot_args = list(colour = "firebrick"),
   whisker_args = list(colour = "firebrick")
   )+
  labs(x = "Timeline (days)", y = "")+
  ggplot2::geom_vline(xintercept = 0, colour = "black", linetype = 2)+
  theme_classic()+
  ggtitle("Regression output: all variables, using only Google scores. N/A \n scores coded as 0:")+
  theme(legend.background = element_rect(colour="grey80"))


#Results: significance found in Google sentiment (0.02972) and capacity (0.08989)
#H_L_W: 0.93917
#
####
## 01/20/18 - MS: with additional text, got more significance on values 
# Results: significance found in Google sentiment (0.02486) and capacity (0.06455)
# H_L_W: 0.84519


```


```{r COmparative dotwhiskerplot}

R1 <- reg_TLD_ZEROES_VS_GNSen_PopDen_MedianIncome_Cap_HL_State
R2 <- reg_TLD_ZEROES_VS_NSen_PopDen_MedianIncome_Cap_HL_State
R3 <- reg_TLD_ZEROES_VS_GSen_PopDen_MedianIncome_Cap_HL_State

# general_everything plot

# unique(regression1$term)
# 
# dwplot(list(tidy(R1),tidy(R2),tidy(R3), conf.int = FALSE)
#        # dot_args = list(colour = "firebrick"), whisker_args = list(colour = "firebrick")
#        )+
#     theme_bw()+
#   labs(x = "timeline")



### does not work unless subset by regression 
regression1_1 <- tidy(R1)  %>% 
  filter(!grepl('^State*', term)) %>% 
  mutate(model = "regression1_G+NU")

regression2_1 <- tidy(R2)  %>% 
  filter(!grepl('^State*', term)) %>% 
  mutate(model = "regression2_NU")

regression3_1 <- tidy(R3)  %>% 
  filter(!grepl('^State*', term)) %>% 
  mutate(model = "regression3_G")

combined_reg <-  bind_rows(regression1_1, regression2_1, regression3_1)
View(combined_reg)


dwplot(combined_reg %>% 
   relabel_predictors(renamed_variables), 
   dot_args = list(aes(shape = model)),
   whisker_args = list(aes(colour = model))
   )+
  labs(x = "Timeline (days)", y = "")+
  ggplot2::geom_vline(xintercept = 0, colour = "grey80", linetype = 2)+
  theme_classic()+
  ggtitle("Combined regression outputs;\n N/A scores coded as 0:")+
  theme(plot.title = element_text(face="bold"),
          legend.position = c(0.65, 0.8),
          legend.justification = c(0, 0),
          legend.background = element_rect(colour = "grey60"),
          legend.title.align = .5)
  
# install.packages("jtools")
# library(jtools)
# plot_summs(tidy(R1), scale = TRUE, plot.distributions = TRUE, inner_ci_level = .9)
#  coefplot()

```


### Sentiment on left hand side - NAs still coded as '0'

# NU sentiment
```{r R11}
####11. NU Sentiment on left-hand side. N/A scores coded as "0"
reg_NSen_VS_PopDen_MedianIncome_Cap_HL_State <- lm(NU_Sentiment ~ View_Score + PopDensity_mi + Household_MedianIncome + Capacity + H_L_W + State, data=ventyx_df_NU_google_Sen)

summary(reg_NSen_VS_PopDen_MedianIncome_Cap_HL_State)

dwplot(reg_NSen_VS_PopDen_MedianIncome_Cap_HL_State)+
  theme_bw()+
  labs(x = "sentiment")

regressionA <- tidy(reg_NSen_VS_PopDen_MedianIncome_Cap_HL_State)

dwplot(regressionA[!grepl("^State*", regressionA$term),] %>% 
   relabel_predictors(renamed_variables), 
   dot_args = list(colour = "firebrick"),
   whisker_args = list(colour = "firebrick", lwd = 0.5)
   )+
  labs(x = "Sentiment score", y = "")+
  ggplot2::geom_vline(xintercept = 0, colour = "black", linetype = 2)+
  theme_classic()+
  ggtitle("Regression output: all variables including NU and Google \n scores, interaction terms. N/A scores coded as 0:")+
  theme(legend.background = element_rect(colour="grey80"))


#Results: significance found in H_L_W (0.0273)
####
# 01/21/18 - difference in results with changes to ventyx df and text from NU:
#Results: significance found in H_L_W (0.0275)


```
# google sentiment
```{r R12}
####12. Google Sentiment on left-hand side. N/A scores coded as "0"
reg_GSen_VS_PopDen_MedianIncome_Cap_HL_State <- lm(Google_Sentiment ~ View_Score + PopDensity_mi + Household_MedianIncome + Capacity + H_L_W + State, data=ventyx_df_NU_google_Sen)

summary(reg_GSen_VS_PopDen_MedianIncome_Cap_HL_State)

# dwplot(reg_GSen_VS_PopDen_MedianIncome_Cap_HL_State)+
#   theme_bw()+
#   labs(x = "timeline")
# 

regressionB <- tidy(reg_GSen_VS_PopDen_MedianIncome_Cap_HL_State)

dwplot(regressionB[!grepl("^State*", regressionB$term),] %>% 
   relabel_predictors(renamed_variables), 
   dot_args = list(colour = "firebrick"),
   whisker_args = list(colour = "firebrick", lwd = 0.5)
   )+
  labs(x = "Sentiment score", y = "")+
  ggplot2::geom_vline(xintercept = 0, colour = "black", linetype = 2)+
  theme_classic()+
  ggtitle("Regression output: all variables including NU and Google \n scores, interaction terms. N/A scores coded as 0:")+
  theme(legend.background = element_rect(colour="grey80"))



#Results no significance found
#H_L_W: 0.154
####
# 01/21/18 - difference in results with changes to ventyx df and text from NU:
#H_L_W: 0.16372
#capacity: 0.11

```

```{r R13}
####13. Timeline on left-hand side, using NU sentiment. Interaction term between NU and high-low impact. Environmental memberships variable added. N/A scores coded as 0
reg_NSen_VS_PopDen_MedianIncome_Cap_HL_State_Memberships_Inter <- lm(TimelineDays ~ View_Score + NU_Sentiment + members_env + PopDensity_mi + Household_MedianIncome + Capacity + H_L_W + State + H_L_W*NU_Sentiment, data=ventyx_df_NU_google_Sen)

summary(reg_NSen_VS_PopDen_MedianIncome_Cap_HL_State_Memberships_Inter)
dwplot(reg_NSen_VS_PopDen_MedianIncome_Cap_HL_State_Memberships_Inter)
```


```{r R14}
####14. Timeline on left-hand side, using Google sentiment. Interaction term between Google and high-low impact. Environmental memberships variable added. N/A scores coded as 0
reg_GSen_VS_PopDen_MedianIncome_Cap_HL_State_Memberships_Inter <- lm(TimelineDays ~ View_Score + Google_Sentiment + members_env + PopDensity_mi + Household_MedianIncome + Capacity + H_L_W + State + H_L_W*Google_Sentiment, data=ventyx_df_NU_google_Sen)

summary(reg_GSen_VS_PopDen_MedianIncome_Cap_HL_State_Memberships_Inter)
dwplot(reg_GSen_VS_PopDen_MedianIncome_Cap_HL_State_Memberships_Inter)
```


## OTHER 

#### Headlines
```{r Extract_headlines_id_duplicates}

#can ignore for regression

## extract headlines 
projects_pdf_titles <- project_pdfs_full_text %>% 
  mutate(title_extract = grepl("\r\n \\d.(.*?) Client/Matter:", text, ignore.case = FALSE)) 
# %>% 
  # filter(title_extract == TRUE) %>% 
  # mutate(title_match = strapply(text, "\r\n \\d.(.*?) Client/Matter:"))


  
## Identify duplicate headlines 
  
  

  
```


#### Populate with negative words 
```{r negative_words}
negative_words <- paste0(c('negative|postpone|against|delay|lawsuit|litigation|protest|^cost|^stop'))
# Function to replace `character(0)` with NAs as NULL values are dropped when flattening list
# inspired by: https://colinfay.me/purrr-set-na/
charnull_set <- function(x) {
  p <- purrr::as_mapper(~identical(., character(0)))
  x[p(x)] <- NA
  return(x)
}
projects_pdftext_3 <- projects_pdftext_NU %>%
  mutate(query_hits = str_extract_all(text, pattern = regex(negative_words, ignore_case=TRUE)) %>%  # Extract all the keywords
           map(~charnull_set(.x)) %>%   # Replace character(0) with NAs
           map_chr(~glue::glue_collapse(.x, sep = ";")) %>%   # collapse the multiple hits
           tolower) # all our keywords are lower case
projects_pdftext_grouped <- projects_pdftext_2 %>%
  group_by(document, query_hits)
# OR 
my_data1grouped <- my_data1 %>%
  group_by(document, query_hits)%>%
  summarise(word_list = glue::glue_collapse(query_hits, sep = ";"))
View(projects_pdftext_2)
```


## Regression where NA's removed 

### NU no interaction
```{r R1}

require(broom)
# install.packages("dotwhisker")
library(dotwhisker)

ventyx_df_NU_Sen$Capacity_1 <- scale(ventyx_df_NU_Sen$Capacity)

View(ventyx_df_NU_Sen)
####1. All variables included -- using NU sentiment scores. N/A scores removed from dataset.
reg_TLD_VS_Sen_PopDen_MedianIncome_Cap_HL_State <- lm(TimelineDays ~ View_Score + NU_Sentiment + PopDensity_mi + Household_MedianIncome + Capacity + H_L_W + State, data=ventyx_df_NU_Sen)
print(reg_TLD_VS_Sen_PopDen_MedianIncome_Cap_HL_State)

NORM_reg_TLD_VS_Sen_PopDen_MedianIncome_Cap_HL_State <- lm(TimelineDays ~ View_Score_2 + NU_Sentiment_2 + PopDensity_mi_2 + Household_MedianIncome_2 + Capacity_2 + H_L_W + State, data=ventyx_df_NU_google_Sen_norm)

summary(NORM_reg_TLD_VS_Sen_PopDen_MedianIncome_Cap_HL_State)

#Plotting residuals. This should be repeated for the other regressions.
r1 <- resid(reg_TLD_VS_Sen_PopDen_MedianIncome_Cap_HL_State)
plot(r1)

summary(reg_TLD_VS_Sen_PopDen_MedianIncome_Cap_HL_State)
require(ggplot2)

View(tidy(reg_TLD_VS_Sen_PopDen_MedianIncome_Cap_HL_State))

dwplot(tidy(reg_TLD_VS_Sen_PopDen_MedianIncome_Cap_HL_State))

#Results: significance found in household median income (0.00358) and capacity (0.00582). 
#NU sentiment p-value: 0.22164
#H_L_W p-value: 0.62724
####

```

### GOogle no interaction
```{r R3}

####3. All variables included -- using Google sentiment scores. N/A scores removed from dataset.
reg_TLD_VS_GSen_PopDen_MedianIncome_Cap_HL_State <- lm(TimelineDays ~ View_Score + Google_Sentiment + PopDensity_mi + Household_MedianIncome + Capacity + H_L_W + State, data=ventyx_df_google_Sen)

summary(reg_TLD_VS_GSen_PopDen_MedianIncome_Cap_HL_State)
dwplot(tidy(reg_TLD_VS_GSen_PopDen_MedianIncome_Cap_HL_State))+
  theme_bw()+
  labs(x = "timeline")
 
#Results: no significance found, although Google sentiment is close
#Google sentiment p-value: 0.1015
#H_L_W p-value: 0.4502
####
```

### NU plus interaction
```{r R_NU+interaction}
##2. All variables included -- using NU sentiment scores and an interaction between the high-low variable and NU sentiment. N/A scores removed from dataset.
reg_TLD_VS_Sen_PopDen_MedianIncome_Cap_HL_State_Inter <- lm(TimelineDays ~ View_Score + NU_Sentiment + PopDensity_mi + Household_MedianIncome + Capacity + H_L_W + State + H_L_W*NU_Sentiment, data=ventyx_df_NU_Sen)

summary(reg_TLD_VS_Sen_PopDen_MedianIncome_Cap_HL_State_Inter)
dwplot(tidy(reg_TLD_VS_Sen_PopDen_MedianIncome_Cap_HL_State_Inter))+
    theme_bw()+
  labs(x = "timeline")
 
# Results: significance found in household median income (0.002891) and capacity (0.004197) and H_L_W (0.04786) and the interaction term (0.03391)
#NU sentiment p-value: 0.82485
####

```

### Google plus interaction
```{r R4}
####4. All variables included -- using Google sentiment scores and an interaction between the high-low variable and Google sentiment. N/A scores removed from dataset.
reg_TLD_VS_GSen_PopDen_MedianIncome_Cap_HL_State_Inter <- lm(TimelineDays ~ View_Score + Google_Sentiment + PopDensity_mi + Household_MedianIncome + Capacity + H_L_W + State + H_L_W*Google_Sentiment, data=ventyx_df_google_Sen)

summary(reg_TLD_VS_GSen_PopDen_MedianIncome_Cap_HL_State_Inter)
dwplot(tidy(reg_TLD_VS_GSen_PopDen_MedianIncome_Cap_HL_State_Inter))+
    theme_bw()+
  labs(x = "timeline")
 
#Results: no significance found, although Google sentiment is close
#Google sentiment p-value: 0.1021
#H_L_W p-value: 0.97
#Interaction: 0.48
#

```







