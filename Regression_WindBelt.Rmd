---
title: "WindBelt Regressions"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

---
title: "WindBelt Regressions"
author: "WindBelt GP"
date: "January 13, 2018"
output: Word Document
---

```{r setup2, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#### Outline of lexisUni text analysis (checked if done):
 1. Dowload pdf from lexisUni - search terms: "Master Project Name" AND "Project Developer" AND "wind" AND "energy' (if necessary, the state was added in the search term if location was not precise enough. 
 2. Save pdf in sample_pdfs folder x
      Each pdf saved as "fulltext"+ # search results if <10" + "abbreved project developer" "# of results pages in         >10", collapsed with "_" x
2. Create a df with everypdf in each row. my_data x
3. Create df with unested text unested,such that each row is a pdf page.x
4. Created new df that splits each page by word, sch that every row is a word in the text (unest_tokens where tokens are pdf) x
5. Get word count of specific words through group_by()
Conduct sentiment analysis on unique words x 
6. get hits and number of hits of different 'negative words' x
# Following meeting 10/22/18:
1. Unest token by group of words or sentences and conduct sentiment analysis on this.
2. Clean scripts - ID words in pdf that consistently pop up and need to be filtered out.
3. separate headlines from text to ensure we don't have duplicates
4. create csv format (NAME, Developer, State, Sentiment, subjectivity ...)
Other notes: 
*tidytext::tokenize function - every element in list become df. rbind dfs 
 str_count() how many times does a search term come up 
 str_match()
 regex() 
 
```{r load_packages}
### Packages 
library(pdftools)
library(tm)
# install.packages("rlang")
# remove.packages("devtools")
# install.packages("devtools")
library(devtools)
library(dplyr)
library(tidytext)
library(broom)
library(data.table)
# install.packages("colorspace")
library(tidyverse)
library(purrr)
library(googledrive)
library(knitr)
library(readr)
library(stringr)
library(gsubfn)
# install.packages("tidyverse")
# install.packages("tidytext")
# library(tm.plugin.lexisnexis)
# install.packages("gsubfn", "dplyr", "tidytext")
```


```{r connect_to_googledrive}

# The google drive folder id is simply the id in the folder url after the last slash
# So, in this example, the id here is derived from https://drive.google.com/drive/folders/1kZuJF3eS7SIiC8VBeGc6vVNvpBZHLnxg?ogsrc=32

# Create folder in desktop for pdfs. Decided to set on desktop to be compatible with all computers.
# If working on a Bren computer, use this:
NU_PDFS_R <- "H:/Desktop/NU_PDFS_R"

# Alex's Directory:
#NU_PDFS_R <- "C:/Users/airvi/Documents/Bren/GroupProject/NU_PDFS_R"

dir.create(NU_PDFS_R, showWarnings = TRUE) # if directory already exists, will give warning. Ignore.

# Pull all pdfs directly from Google Drive 
NU_PDFs_R_id  <- "1alXSN-uUouUNM2cTHq5OS3LSxVhSDa_v"
NU_PDFs_R_folder <- googledrive::drive_ls(googledrive::as_id(NU_PDFs_R_id))

# function to download all lexisUni pdfs
pdf_downloader <- function(templates_dribble, local_folder){
  # download all pdfs
  
  for (i in 1:nrow(templates_dribble)){
    drive_download(as_id(templates_dribble$id[[i]]), 
                   file.path(local_folder, templates_dribble$name[[i]]),
                   overwrite = F) #check if overwrite is needed here
  }
}

pdf_downloader(NU_PDFs_R_folder, NU_PDFS_R)
  #function takes a while, since its pulling all 349 pdfs from googledrive


## 
# WindBelt_Data <- drive_download(as_id("https://docs.google.com/spreadsheets/d/1obj9RRgbt9XGudtZW-v_7QNiOuWrN3gdSb9RKmRBgco/edit#gid=146899731"), path = "ventyx_converted_11_27_2018_PopDensity_Income_Viewshed_lowhighimpact_doc_names", type = "csv", overwrite = TRUE)

#WindBelt_Full <- read.csv("~/Downloads/ventyx_converted_11_27_2018_PopDensity_Income_Viewshed_lowhighimpact_doc_names.csv")
#View(WindBelt_Full)

```



```{r import_data}
##### Ventyx ####
#Before running this code, make sure you have the Ventyx dataset saved on your desktop. The file path that the "read_csv" function accesses should be where the file is located on your desktop.


##Pulling Ventyx from Google drive (TBC)

  # ventyx_id <- "1lvoVfdeUjft6sFtXwZ7OZo5ly3B7VZ9g?ogsrc=32"
  # 
  # Windbelt_data_folder <- googledrive::drive_ls(googledrive::as_id(ventyx_id))
  # 
  # drive_download(file = "ventyx_converted_01_08_2019_PopDensity_Income_Viewshed_lowhighimpact_doc_names_IncomeMedian.csv", path = Windbelt_data_folder), 
  #                    file.path(local_folder, templates_dribble$name[[i]]),
  #                    overwrite = FALSE)
  # 
  # WindBelt_Data <- drive_download(as_id("https://docs.google.com/spreadsheets/d/1obj9RRgbt9XGudtZW-v_7QNiOuWrN3gdSb9RKmRBgco/edit#gid=146899731"), path = "ventyx_converted_11_27_2018_PopDensity_Income_Viewshed_lowhighimpact_doc_names", type = "csv", overwrite = TRUE)


#For MAC and BREN computers:
ventyx_projects <- read_csv("~/Desktop/ventyx_converted_01_08_2019_PopDensity_Income_Viewshed_lowhighimpact_doc_names_IncomeMedian.csv")

#For Alex:
#ventyx_projects <- read_csv("C:/Users/airvi/Desktop/ventyx_converted_01_08_2019_PopDensity_Income_Viewshed_lowhighimpact_doc_names_IncomeMedian.csv")

#To view the dataset:
#View(ventyx_projects)

#To change the 0-1 definition to Low-High
ventyx_df <- mutate(ventyx_projects, H_L_W = ifelse(lr_tif>0, "Low", "High"))

#Remove whitespace for name matching later one
ventyx_df$ProjectName <- gsub(" ", "", ventyx_df$ProjectName, fixed = TRUE)
ventyx_df$document <- gsub(" ", "", ventyx_df$document, fixed = TRUE)

#### Google News ######
#Before running this code, make sure you have the Google scraping dataset saved on your desktop. The file path that the "read_csv" function accesses should be where the file is located on your desktop. 

#For MAC and BREN computers:
google_df <- read_csv("~/Desktop/google_scraping_01-08-19.csv")

#For Alex:
#google_df <- read_csv("C:/Users/airvi/Desktop/google_scraping_01-08-19.csv")

#To view the dataset:
#View(google_df)
```



```{r nexus_directory_setup}
#For MAC and BREN computers:
pdf_directory <- '~/Desktop/NU_PDFS_R'

#For Alex:
#pdf_directory <- "C:/Users/airvi/Documents/Bren/GroupProject/NU_PDFS_R"

#For Delaney:
#pdf_directory <- "D:/Desktop/All_LexisUni_PDFs"
#pdf_directory <- "~/Desktop/All_LexisUni_PDFs"

#Listing all PDFs: should be 349 PDFs
pdfs <- paste(pdf_directory, "/", list.files(pdf_directory, pattern = "*.pdf", ignore.case = T), sep = "")

#PDF names
pdfs_names <- list.files(pdf_directory, pattern = "*.pdf", ignore.case = T)

#PDF text
pdfs_text <- purrr::map(pdfs, pdftools::pdf_text)

  #Takes a minute...
  #Expect 9 'PDF error' and ignore. Text normally still processed
```

#### Create initial dataframes that include document name and text
```{r dataframe_creation}

##### NU ####
#This combines the pdfs_names and pdfs_texts variables from the previous code chunk, into a single dataframe
#Each row is a PDF doc name with the full pdf text. Note: in the text column, each row is an element of a list
projects_NU <- data_frame(document = pdfs_names, text = pdfs_text)

#### Google ####
#This creates a data frame of just the project name and full text for each Google News article
#Each row is the project name with the full article text
projects_google <- data_frame(document = google_df$ProjectName, text = google_df$FullText)

```

#### Split text by page (only NU)
```{r page_split_aggregate}
#Dataset with each page in one row
project_pdfpages_NU <- projects_NU %>% 
  unnest() # splits pdf text by page and removes list format ( c("")) since each element is now its own row.

#Collapse pages so that every row under text is the full pdf. Chose to indicate page separation by (/page)
project_pdfs_full_text <- project_pdfpages_NU %>%
  group_by(document) %>%
  summarise(text = paste(text, collapse = " (/p) "))
    # note: if you write this to a csv, the next will go the next line and won't look like a clean csv (i.e. Alex's google news csv). TBc.

# View(head(projects_NU))
# View(head(project_pdfpages_NU))
# View(head(project_pdfs_full_text))

```

#### Split text by word (unnest_tokens())
```{r word_split}
#### NU ####
#Dataset with each word in a row associated with its pdf source
#Also filters out unwanted words

projects_words_NU <- project_pdfs_full_text %>%
  tidytext::unnest_tokens(output = word, input = text, token = 
                          "words", to_lower = T) %>%   # important to put all words to lower because it AFINN dictionnary has words only in lower case format     
  filter(!word %in% c("lexis",
                      "nexis", 
                      "Uni",
                      "about lexisnexis",
                      "Privacy Policy",
                      "Terms & Conditions", 
                      "Copyright ? 2018 LexisNexis",
                      " | ",  
                      "@", 
                      "lexisnexis", "(/p)"))

#head(projects_words_NU)
length(unique(projects_words_NU$document)) # results: still 349 projects here after word parsing
# 
# projects_words_NU <- projects_NU %>% 
#   unnest() %>% 
#   tidytext::unnest_tokens(output = word, input = text, token = 
#                           "words", to_lower = T) %>%      
#   filter(!word %in% c("lexis",
#                       "nexis", 
#                       "Uni",
#                       "about lexisnexis",
#                       "Privacy Policy",
#                       "Terms & Conditions", 
#                       "Copyright Â© 2018 LexisNexis",
#                       " | ",  
#                       "@", 
#                       "lexisnexis")) 

# %>% gsub("[^A-Za-z0-9,;._-]","")

#projects_pdfnest_NU <- projects_pdftext %>% 
#  unnest() %>% 
#  tidytext::unnest_tokens(output = ngrams, input = text, token = "ngrams", n = 5, to_lower = T)

# note: unnest_tokens() splits text by respective element (ie word, phrase, ...) word is default


#### Google #####
#Dataset with each word in a row associated with its project source 
projects_words_google <- projects_google %>% 
  tidytext::unnest_tokens(output = word, input = text, token = "words", to_lower = F)
head(projects_words_google)

```


#### Group words by pdf/project and summarize by frequency
```{r group_by}

#### NU ####
projects_words_count_NU <- projects_words_NU %>%
  group_by(document, word) %>% 
  summarise(count = n())
#View(projects_pdfwords_count)
#Counts the number of time a specific words is found in the pdf page

length(unique(projects_words_count_NU$document)) # results: there are still 349 projects here 

# projects_pdfnest_count <- projects_pdfnest_NU %>%
#   group_by(document, ngrams) %>% 
#   summarise(count = n())
#View(projects_pdfnest_count)
#add new count column with most freq. words


#### Google ####
projects_words_count_google <- projects_words_google %>%
  group_by(document, word) %>% 
  summarise(count = n())
#View(projects_pdfwords_count)
#Counts the number of time a specific words is found in the article
```

#### Sentiment dictionaries
```{r sentiment_dictionaries}
# Using 'afinn' vs. 'nrc sentiment tests.

get_sentiments("afinn") # associates word with a sentiment score
#afinn scores/ranks from -5 to +5 for positive or negative sentiment.

get_sentiments("nrc") # associated word with another sentiment feeling word

# View(get_sentiments("afinn"))
# View(get_sentiments("nrc"))

# We want scores not categorized words, so we will use AFINN for now.
```

#### Bind Sentiments
```{r bind_sentiment}
#### NU ####
projects_score_bind_NU <-projects_words_count_NU %>% 
  left_join(get_sentiments("afinn"), by = "word") %>% 
  # filter(score != "NA") %>% 
  filter(!is.na(score)) 
# Note: Many of the scores per words are NA simply because that word does not exist. 
  
unique(projects_score_bind_NU$document)

# look at which projects have NA all NA's as words
projects_score_bind_NU_NA <-projects_words_count_NU %>% 
  left_join(get_sentiments("afinn"), by = "word") %>% 
  filter(is.na(score)) 


length(unique(projects_score_bind_NU$document)) # 349
### general examples

## projects with few words
projects_score_bind_NU %>% 
  filter(document == "Allendorf_NorthernAlternativeEnergy_1(2).PDF")

projects_score_bind_NU %>% 
  filter(document == "GreatPathfinderWind_ApexCleanEnergy_1(4).PDF")

## more words
projects_score_bind_NU %>% 
  filter(document == "ArranzWindTuxedo_Energos_1.PDF")

#### Google ####
projects_score_bind_google <- projects_words_count_google %>% 
  left_join(get_sentiments("afinn"), by = "word") %>% 
  filter(score != "NA")

#View(projects_score_bind)

# Note: Many of the scores per words are NA simply because that word does not exist. 
```

#### Determine Project Scores
```{r projectscores}

#To determine the total score for each document (NU) or project (Google)

# View(ventyx_df)

##### NU ######
total_sentiment_with_stats_NU <- projects_score_bind_NU %>% 
  #filter(score !="NA") %>% 
  group_by(document) %>% 
  summarise(totals = weighted.mean(score, w = count),
            standard_dev = sd(score), 
            variance = var(score))

total_sentiment_NU <- projects_score_bind_NU %>% 
  #filter(score !="NA") %>% 
  group_by(document) %>% 
  summarise(totals = mean(score))
  
#View(total_sentiment_with_stats_NU)
View(total_sentiment_NU)
length(unique(total_sentiment_NU$document)) # --> 349

#### Google #####
total_sentiment_with_stats_google <- projects_score_bind_google %>% 
  #filter(score !="NA") %>% 
  group_by(document) %>% 
  summarise(totals = weighted.mean(score, w = count),
            standard_dev = sd(score), 
            variance = var(score)
            )

total_sentiment_google <- projects_score_bind_google %>% 
  #filter(score !="NA") %>% 
  group_by(document) %>% 
  summarise(totals = mean(score))
  
#View(total_sentiment_with_stats_google)
#View(total_sentiment_google)
```

####Combine sentiment scores with original data
```{r combinewithsen}

#### NU ####

#Convert to data frame
total_sentiment_NU_df = as.data.frame(total_sentiment_NU)
# View(total_sentiment_NU_df)
# View(ventyx_df)
# To ensure the project name from Ventyx matches properly with the document name in total_sentiment_NU

# Remove white space in document names in ventyx and total_sentiment_NU_df


# mutate new column with project names
# total_sentiment_NU_df <- total_sentiment_NU_df %>%
#   mutate(ProjectName = sub("\\_.*$", "", document)) # extract words before first '_' in document name

#NU
total_sentiment_NU_df$document <- gsub(" ", "", total_sentiment_NU_df$document)
#Ventyx 
# ventyx_df$document <- gsub(" ", "", ventyx_df$document)
  
# View(ventyx_df)
# View(total_sentiment_NU_df)

#Merge with original data (ventyx_df)
# 
# View(ventyx_df)
# View(total_sentiment_NU_df)
# 
# a <- intersect(ventyx_df$document, total_sentiment_NU_df$document) 
# setdiff(total_sentiment_NU_df$document, a)

ventyx_df[ventyx_df$ProjectName == "71RWind",]$document = "71RWind_71Ranch_1(1).PDF"
ventyx_df[ventyx_df$ProjectName == "CHillWindFarm",]$document = "CHill_ExergyIntegratedSystems_1.PDF"
ventyx_df[ventyx_df$ProjectName == "AdamsProject",]$document = "AdamsProject_MidAmerican_1(2).PDF"
ventyx_df[ventyx_df$ProjectName == "LJTrust",]$document = "LJTrust_LJTrust_1(1).PDF"
ventyx_df[ventyx_df$ProjectName == "MinonkWindFarm",]$document = "MinonkWindFarm_MinonkWindFarm_1.PDF"

  
ventyx_df_NU_Sen <- merge(ventyx_df, total_sentiment_NU_df, by = "document", all = TRUE)
# ventyx_df_NU_Sen <- merge(ventyx_df, total_sentiment_NU_df, by = "document", all = TRUE) #old

#Rename "totals" column
ventyx_df_NU_Sen <- rename(ventyx_df_NU_Sen, NU_Sentiment=totals)

# View(ventyx_df_NU_Sen)
length(unique(ventyx_df_NU_Sen$document))
length(unique(total_sentiment_NU_df$document))


#### Google #####
#Convert to data frame
total_sentiment_google_df = as.data.frame(total_sentiment_google)
#Remove white space in order to perform merge (google data has extra white spaces)
total_sentiment_google_df$document <- gsub(" ", "", total_sentiment_google_df$document, fixed = TRUE)

# View(total_sentiment_google_df)


#Merge with original data
ventyx_df_google_Sen <- merge(ventyx_df, total_sentiment_google_df, by.x ="ProjectName", by.y = "document", all = TRUE)
#Rename "totals" columns
ventyx_df_google_Sen <- rename(ventyx_df_google_Sen, Google_Sentiment=totals)

# View(ventyx_df_google_Sen)
# names(total_sentiment_google_df)
# names(ventyx_df_NU_Sen)
# ventyx_df_NU_Sen$
#### Combined ####

ventyx_df_NU_google_Sen <- merge(ventyx_df_NU_Sen, total_sentiment_google_df, by.x = "ProjectName", by.y = "document", all = TRUE)

ventyx_df_NU_google_Sen <- ventyx_df_NU_google_Sen %>% 
  rename(Google_Sentiment=totals) %>% 
  mutate(NU_Sentiment = ifelse(is.na(NU_Sentiment),0,NU_Sentiment)) %>% 
  mutate(Google_Sentiment = ifelse(is.na(Google_Sentiment),0,Google_Sentiment))


ventyx_df_NU_google_Sen <- ventyx_df_NU_google_Sen %>% 
  filter(Capacity >= 0) %>% 
  filter(TimelineDays > 0)

unique(ventyx_df_NU_google_Sen$document)

View(ventyx_df_NU_google_Sen)

#write.csv(ventyx_df_NU_google_Sen, "~/Desktop/ventyx_df_NU_google_Sen_01.18.19.csv")


# names(ventyx_df_google_Sen)
# View(total_sentiment_NU_df)
# View(ventyx_df_NU_google_Sen)


```

####Convert sentiment scores to positive or negative in new column (optional)
```{r pos_or_neg_sen}

ventyx_df_NU_Sen <- mutate(ventyx_df_NU_Sen, Sign_NU = ifelse(NU_Sentiment>0, "Positive", "Negative"))
ventyx_df_google_Sen <- mutate(ventyx_df_google_Sen, Sign_Google = ifelse(Google_Sentiment>0, "Positive", "Negative"))
ventyx_df_NU_google_Sen <- ventyx_df_NU_google_Sen %>% 
  mutate(Sign_Google = ifelse(Google_Sentiment>=0, "Positive", "Negative")) %>% 
  mutate(Sign_NU = ifelse(NU_Sentiment>=0, "Positive", "Negative"))

#View(ventyx_df_NU_Sen)
#View(ventyx_df_google_Sen)
#View(ventyx_df_NU_google_Sen)

```

####Add environmental memberships data
```{r env_members}
#For Alex
#zip_conversion <- read_csv("C:/Users/airvi/Documents/Bren/GroupProject/zip_code_database.csv")

#For Bren computers
zip_conversion <- read_csv("~/Desktop/zip_code_database.csv")

#For Alex
#memberships <- read_csv("C:/Users/airvi/Documents/Bren/GroupProject/membership_data_combined.csv")

#For Bren computers
memberships <- read_csv("~/Desktop/membership_data_combined.csv")

#Make sure all zip codes are 5 digits
memberships$zip <- sprintf("%05d", memberships$zip)

#Merge memberships data with county
zip_county_merge <- merge(zip_conversion, memberships, by = "zip")

#Remove the word county, parish, and municipality, then trim any trailing whitespace
zip_county_merge$county <- gsub("County","",zip_county_merge$county)
zip_county_merge$county <- gsub("Parish","",zip_county_merge$county)
zip_county_merge$county <- gsub("Municipality","",zip_county_merge$county)
zip_county_merge$county <- trimws(zip_county_merge$county, "r")

#Group by county and sum all of the members for environmental organizations
zip_county_merge_aggregate <- zip_county_merge %>% 
  group_by(county) %>% 
  summarise(members_env = sum(members_nrdc,members_nwf,members_tnc))

#Merge with ventyx dataset and set NA membership values to zero
ventyx_df_NU_google_Sen_Memb <- merge(ventyx_df_NU_google_Sen, zip_county_merge_aggregate, by.x = "County", by.y = "county", all.x = TRUE)
ventyx_df_NU_google_Sen_Memb$members_env[is.na(ventyx_df_NU_google_Sen_Memb$members_env)] <- 0
```


####Prepare the relevant columns for regression
```{r organize_for_regressions}
#na.omit(ventyx_df_NU_Sen$Household_MeanIncome)
#na.omit(ventyx_df_NU_Sen$PopDensity_mi)
#na.omit(ventyx_df_NU_Sen$lr_tif)
#na.omit(ventyx_df_NU_Sen$View_Score)

#### NU ####
ventyx_df_NU_Sen$H_L_W <- as.factor(ventyx_df_NU_Sen$H_L_W)

ventyx_df_NU_Sen$PopDensity_mi <- as.numeric(ventyx_df_NU_Sen$PopDensity_mi)

ventyx_df_NU_Sen$Household_MedianIncome <- as.numeric(ventyx_df_NU_Sen$Household_MedianIncome)

ventyx_df_NU_Sen$View_Score <- as.numeric(ventyx_df_NU_Sen$View_Score)

ventyx_df_NU_Sen$H_L_W <- relevel(ventyx_df_NU_Sen$H_L_W, ref = "High")

ventyx_df_NU_Sen$Sign_NU <- as.factor(ventyx_df_NU_Sen$Sign_NU)

#str(ventyx_df_NU_Sen)

#summary(ventyx_df_NU_Sen)

#### Google ####
ventyx_df_google_Sen$H_L_W <- as.factor(ventyx_df_google_Sen$H_L_W)

ventyx_df_google_Sen$PopDensity_mi <- as.numeric(ventyx_df_google_Sen$PopDensity_mi)

ventyx_df_google_Sen$Household_MedianIncome <- as.numeric(ventyx_df_google_Sen$Household_MedianIncome)

ventyx_df_google_Sen$View_Score <- as.numeric(ventyx_df_google_Sen$View_Score)

ventyx_df_google_Sen$H_L_W <- relevel(ventyx_df_google_Sen$H_L_W, ref = "High")

ventyx_df_google_Sen$Sign_Google <- as.factor(ventyx_df_google_Sen$Sign_Google)

#str(ventyx_df_google_Sen)

#summary(ventyx_df_google_Sen)

#### Combined ####
ventyx_df_NU_google_Sen_Memb$H_L_W <- as.factor(ventyx_df_NU_google_Sen_Memb$H_L_W)

ventyx_df_NU_google_Sen_Memb$PopDensity_mi <- as.numeric(ventyx_df_NU_google_Sen_Memb$PopDensity_mi)

ventyx_df_NU_google_Sen_Memb$Household_MedianIncome <- as.numeric(ventyx_df_NU_google_Sen_Memb$Household_MedianIncome)

ventyx_df_NU_google_Sen_Memb$View_Score <- as.numeric(ventyx_df_NU_google_Sen_Memb$View_Score)

ventyx_df_NU_google_Sen_Memb$H_L_W <- relevel(ventyx_df_NU_google_Sen_Memb$H_L_W, ref = "High")

ventyx_df_NU_google_Sen_Memb$Sign_Google <- as.factor(ventyx_df_NU_google_Sen_Memb$Sign_Google)

ventyx_df_NU_google_Sen_Memb$Sign_NU <- as.factor(ventyx_df_NU_google_Sen_Memb$Sign_NU)

str(ventyx_df_NU_google_Sen_Memb)

summary(ventyx_df_NU_google_Sen_Memb)
```


####Run Regressions
```{r R1}
####1. All variables included -- using NU sentiment scores. N/A scores removed from dataset.
reg_TLD_VS_Sen_PopDen_MedianIncome_Cap_HL_State <- lm(TimelineDays ~ View_Score + NU_Sentiment + PopDensity_mi + Household_MedianIncome + Capacity + H_L_W + State, data=ventyx_df_NU_Sen)
print(reg_TLD_VS_Sen_PopDen_MedianIncome_Cap_HL_State)

#Plotting residuals. This should be repeated for the other regressions.
r1 <- resid(reg_TLD_VS_Sen_PopDen_MedianIncome_Cap_HL_State)
plot(r1)

summary(reg_TLD_VS_Sen_PopDen_MedianIncome_Cap_HL_State)
require(ggplot2)


install.packages("dotwhisker")
require(dotwhisker)
dwplot(reg_TLD_VS_Sen_PopDen_MedianIncome_Cap_HL_State)
#Results: significance found in household median income (0.00358) and capacity (0.00582). 
#NU sentiment p-value: 0.22164
#H_L_W p-value: 0.62724
####
```



```{r R2}
##2. All variables included -- using NU sentiment scores and an interaction between the high-low variable and NU sentiment. N/A scores removed from dataset.
reg_TLD_VS_Sen_PopDen_MedianIncome_Cap_HL_State_Inter <- lm(TimelineDays ~ View_Score + NU_Sentiment + PopDensity_mi + Household_MedianIncome + Capacity + H_L_W + State + H_L_W*NU_Sentiment, data=ventyx_df_NU_Sen)

summary(reg_TLD_VS_Sen_PopDen_MedianIncome_Cap_HL_State_Inter)
dwplot(reg_TLD_VS_Sen_PopDen_MedianIncome_Cap_HL_State_Inter)+
    theme_bw()+
  labs(x = "timeline")
 
#Results: significance found in household median income (0.002891) and capacity (0.004197) and H_L_W (0.04786) and the interaction term (0.03391)
#NU sentiment p-value: 0.82485
####
```

```{r R3}

####3. All variables included -- using Google sentiment scores. N/A scores removed from dataset.
reg_TLD_VS_GSen_PopDen_MedianIncome_Cap_HL_State <- lm(TimelineDays ~ View_Score + Google_Sentiment + PopDensity_mi + Household_MedianIncome + Capacity + H_L_W + State, data=ventyx_df_google_Sen)

summary(reg_TLD_VS_GSen_PopDen_MedianIncome_Cap_HL_State)
dwplot(reg_TLD_VS_GSen_PopDen_MedianIncome_Cap_HL_State)+
  theme_bw()+
  labs(x = "timeline")
 
#Results: no significance found, although Google sentiment is close
#Google sentiment p-value: 0.1015
#H_L_W p-value: 0.4502
####
```


```{r R4}
####4. All variables included -- using Google sentiment scores and an interaction between the high-low variable and Google sentiment. N/A scores removed from dataset.
reg_TLD_VS_GSen_PopDen_MedianIncome_Cap_HL_State_Inter <- lm(TimelineDays ~ View_Score + Google_Sentiment + PopDensity_mi + Household_MedianIncome + Capacity + H_L_W + State + H_L_W*Google_Sentiment, data=ventyx_df_google_Sen)

summary(reg_TLD_VS_GSen_PopDen_MedianIncome_Cap_HL_State_Inter)
dwplot(reg_TLD_VS_GSen_PopDen_MedianIncome_Cap_HL_State_Inter)+
    theme_bw()+
  labs(x = "timeline")
 
#Results: no significance found, although Google sentiment is close
#Google sentiment p-value: 0.1021
#H_L_W p-value: 0.97
#Interaction: 0.48
####
```

```{r R5}
####5. All variables included -- using both NU and Google scores. N/A scores coded as "0".
reg_TLD_ZEROES_VS_GNSen_PopDen_MedianIncome_Cap_HL_State <- lm(TimelineDays ~ View_Score + Google_Sentiment + NU_Sentiment + PopDensity_mi + Household_MedianIncome + Capacity + H_L_W + State, data=ventyx_df_NU_google_Sen_Memb)

summary(reg_TLD_ZEROES_VS_GNSen_PopDen_MedianIncome_Cap_HL_State)
dwplot(reg_TLD_ZEROES_VS_GNSen_PopDen_MedianIncome_Cap_HL_State)
#Results: significance found in Google sentiment (0.02867) and capacity (0.09094)
#H_L_W p-value: 0.91039
#NU_Sentiment: 0.57313
####
```

```{r R6}
####6. All variables included -- using only NU scores. N/A scores coded as "0".
reg_TLD_ZEROES_VS_NSen_PopDen_MedianIncome_Cap_HL_State <- lm(TimelineDays ~ View_Score + NU_Sentiment + PopDensity_mi + Household_MedianIncome + Capacity + H_L_W + State, data=ventyx_df_NU_google_Sen_Memb)

summary(reg_TLD_ZEROES_VS_NSen_PopDen_MedianIncome_Cap_HL_State)

dwplot(reg_TLD_ZEROES_VS_NSen_PopDen_MedianIncome_Cap_HL_State)
#Results: significance found in capacity (0.07004)
#NU_Sentiment: 0.986
#H_L_W: 0.856
```

```{r R7}
####7. All variables included -- using only NU scores and an interaction between high-low and NU sentiment. N/A scores coded as "0".
reg_TLD_ZEROES_VS_NSen_PopDen_MedianIncome_Cap_HL_State_Inter <- lm(TimelineDays ~ View_Score + NU_Sentiment + PopDensity_mi + Household_MedianIncome + Capacity + H_L_W + State + H_L_W*NU_Sentiment, data=ventyx_df_NU_google_Sen_Memb)

summary(reg_TLD_ZEROES_VS_NSen_PopDen_MedianIncome_Cap_HL_State_Inter)
dwplot(reg_TLD_ZEROES_VS_NSen_PopDen_MedianIncome_Cap_HL_State_Inter)
#Results: significance found in capacity (0.07323)
#NU_Sentiment: 0.505
#H_L_W: 0.643
#Interaction: 0.26455
```

```{r R8}
####8. All variables included -- using only Google scores. N/A scores coded as "0"
reg_TLD_ZEROES_VS_GSen_PopDen_MedianIncome_Cap_HL_State <- lm(TimelineDays ~ View_Score + Google_Sentiment + PopDensity_mi + Household_MedianIncome + Capacity + H_L_W + State, data=ventyx_df_NU_google_Sen_Memb)

summary(reg_TLD_ZEROES_VS_GSen_PopDen_MedianIncome_Cap_HL_State)
dwplot(reg_TLD_ZEROES_VS_GSen_PopDen_MedianIncome_Cap_HL_State)+
  theme_bw()+
  labs(x = "timeline")
  
#Results: significance found in Google sentiment (0.02972) and capacity (0.08989)
#H_L_W: 0.93917
####
```

```{r R9}

####9. All variables included -- using only Google scores and an interaction between high-low and Google sentiment. N/A scores coded as "0"
reg_TLD_ZEROES_VS_GSen_PopDen_MedianIncome_Cap_HL_State_Inter <- lm(TimelineDays ~ View_Score + Google_Sentiment + PopDensity_mi + Household_MedianIncome + Capacity + H_L_W + State + H_L_W*Google_Sentiment, data=ventyx_df_NU_google_Sen_Memb)

summary(reg_TLD_ZEROES_VS_GSen_PopDen_MedianIncome_Cap_HL_State_Inter)
dwplot(reg_TLD_ZEROES_VS_GSen_PopDen_MedianIncome_Cap_HL_State_Inter)
#Results: significance found in Google sentiment (0.01327) and capacity (0.09445)
#H_L_W: 0.63589
#Interaction: 0.204
####
```

```{r R10}
####10. All variables included -- using both NU and Google scores and an interaction between the high-low variable and both NU and Google sentiment. N/A scores coded as "0"
reg_TLD_ZEROES_VS_GNSen_PopDen_MedianIncome_Cap_HL_State_Inter <- lm(TimelineDays ~ View_Score + Google_Sentiment + NU_Sentiment + PopDensity_mi + Household_MedianIncome + Capacity + H_L_W + State + H_L_W*Google_Sentiment + H_L_W*NU_Sentiment, data=ventyx_df_NU_google_Sen_Memb)

summary(reg_TLD_ZEROES_VS_GNSen_PopDen_MedianIncome_Cap_HL_State_Inter)
dwplot(reg_TLD_ZEROES_VS_GNSen_PopDen_MedianIncome_Cap_HL_State_Inter)
#Results: significance found in Google sentiment (0.0067)
#NU_Sentiment: 0.196
#H_L_W: 0.804
#Interaction Google: 0.10846
#Interaction NU: 0.15778
####
```

```{r R11}
####11. NU Sentiment on left-hand side. N/A scores coded as "0"
reg_NSen_VS_PopDen_MedianIncome_Cap_HL_State <- lm(NU_Sentiment ~ View_Score + PopDensity_mi + Household_MedianIncome + Capacity + H_L_W + State, data=ventyx_df_NU_google_Sen_Memb)

summary(reg_NSen_VS_PopDen_MedianIncome_Cap_HL_State)
dwplot(reg_NSen_VS_PopDen_MedianIncome_Cap_HL_State)
#Results: significance found in H_L_W (0.0273)
####
```

```{r R12}
####12. Google Sentiment on left-hand side. N/A scores coded as "0"
reg_GSen_VS_PopDen_MedianIncome_Cap_HL_State <- lm(Google_Sentiment ~ View_Score + PopDensity_mi + Household_MedianIncome + Capacity + H_L_W + State, data=ventyx_df_NU_google_Sen_Memb)

summary(reg_GSen_VS_PopDen_MedianIncome_Cap_HL_State)
dwplot(reg_GSen_VS_PopDen_MedianIncome_Cap_HL_State)
#Results no significance found
#H_L_W: 0.154
####

```

```{r R13}
####13. Timeline on left-hand side, using NU sentiment. Interaction term between NU and high-low impact. Environmental memberships variable added. N/A scores coded as 0
reg_NSen_VS_PopDen_MedianIncome_Cap_HL_State_Memberships_Inter <- lm(TimelineDays ~ View_Score + NU_Sentiment + members_env + PopDensity_mi + Household_MedianIncome + Capacity + H_L_W + State + H_L_W*NU_Sentiment, data=ventyx_df_NU_google_Sen_Memb)

summary(reg_NSen_VS_PopDen_MedianIncome_Cap_HL_State_Memberships_Inter)
dwplot(reg_NSen_VS_PopDen_MedianIncome_Cap_HL_State_Memberships_Inter)
```


```{r R14}
####14. Timeline on left-hand side, using Google sentiment. Interaction term between Google and high-low impact. Environmental memberships variable added. N/A scores coded as 0
reg_GSen_VS_PopDen_MedianIncome_Cap_HL_State_Memberships_Inter <- lm(TimelineDays ~ View_Score + Google_Sentiment + members_env + PopDensity_mi + Household_MedianIncome + Capacity + H_L_W + State + H_L_W*Google_Sentiment, data=ventyx_df_NU_google_Sen_Memb)

summary(reg_GSen_VS_PopDen_MedianIncome_Cap_HL_State_Memberships_Inter)
dwplot(reg_GSen_VS_PopDen_MedianIncome_Cap_HL_State_Memberships_Inter)
```


## OTHER 

#### Headlines
```{r Extract_headlines_id_duplicates}

#can ignore for regression

## extract headlines 
projects_pdf_titles <- project_pdfs_full_text %>% 
  mutate(title_extract = grepl("\r\n \\d.(.*?) Client/Matter:", text, ignore.case = FALSE)) 
# %>% 
  # filter(title_extract == TRUE) %>% 
  # mutate(title_match = strapply(text, "\r\n \\d.(.*?) Client/Matter:"))


  
## Identify duplicate headlines 
  
  

  
```


#### Populate with negative words 
```{r negative_words}
negative_words <- paste0(c('negative|postpone|against|delay|lawsuit|litigation|protest|^cost|^stop'))
# Function to replace `character(0)` with NAs as NULL values are dropped when flattening list
# inspired by: https://colinfay.me/purrr-set-na/
charnull_set <- function(x) {
  p <- purrr::as_mapper(~identical(., character(0)))
  x[p(x)] <- NA
  return(x)
}
projects_pdftext_3 <- projects_pdftext_NU %>%
  mutate(query_hits = str_extract_all(text, pattern = regex(negative_words, ignore_case=TRUE)) %>%  # Extract all the keywords
           map(~charnull_set(.x)) %>%   # Replace character(0) with NAs
           map_chr(~glue::glue_collapse(.x, sep = ";")) %>%   # collapse the multiple hits
           tolower) # all our keywords are lower case
projects_pdftext_grouped <- projects_pdftext_2 %>%
  group_by(document, query_hits)
# OR 
my_data1grouped <- my_data1 %>%
  group_by(document, query_hits)%>%
  summarise(word_list = glue::glue_collapse(query_hits, sep = ";"))
View(projects_pdftext_2)
```